{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:26:29.656705775Z",
     "start_time": "2023-12-13T00:26:27.973770077Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from probabilistic_classifier.experiment import ccmi_experiment, multiclass_probabilistic_classifier_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "prime = None\n",
    "data_range = None\n",
    "num_of_samples = 20000\n",
    "\n",
    "hidden_size_arr = [256, 128, 64]\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "num_of_outer_iteration = 10\n",
    "num_of_mid_iteration = 5\n",
    "num_of_inner_iteration = int(20 * num_of_samples / batch_size)\n",
    "\n",
    "para_param, priv_param = None, None\n",
    "beta_arr, alpha_arr = None, None\n",
    "\n",
    "feature_size = None\n",
    "weight = None\n",
    "z_dim = 200\n",
    "x_idx, y_idx, z_idx = [0], [1], list(range(2, z_dim + 2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:39:12.826092778Z",
     "start_time": "2023-12-13T00:39:12.785569054Z"
    }
   },
   "id": "b8bb10f2344d44d1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6901969909667969, avg loss: 0.6929287972450257\n",
      "trial: 1, iter: 1000, curr loss: 0.6972821950912476, avg loss: 0.6923348042964935\n",
      "trial: 1, iter: 1500, curr loss: 0.6914111971855164, avg loss: 0.6916990562677383\n",
      "trial: 1, iter: 2000, curr loss: 0.6940567493438721, avg loss: 0.6910545303821564\n",
      "trial: 1, iter: 2500, curr loss: 0.6827295422554016, avg loss: 0.6895940698385239\n",
      "trial: 1, iter: 3000, curr loss: 0.6796845197677612, avg loss: 0.6868662360906601\n",
      "trial: 1, iter: 3500, curr loss: 0.6831839680671692, avg loss: 0.6836462349891662\n",
      "trial: 1, iter: 4000, curr loss: 0.6689622402191162, avg loss: 0.6800556463003159\n",
      "trial: 1, iter: 4500, curr loss: 0.6798208355903625, avg loss: 0.6767751858234405\n",
      "trial: 1, iter: 5000, curr loss: 0.6605769991874695, avg loss: 0.6728285796642304\n",
      "trial: 1, iter: 5500, curr loss: 0.6573387384414673, avg loss: 0.6689871517419815\n",
      "trial: 1, iter: 6000, curr loss: 0.6329872608184814, avg loss: 0.6639957981109619\n",
      "trial: 1, ldr: -0.2793663740158081, dv: -0.636553168296814, nwj: -0.7086691856384277\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6903850436210632, avg loss: 0.6929937324523926\n",
      "trial: 2, iter: 1000, curr loss: 0.6953601837158203, avg loss: 0.6924981280565262\n",
      "trial: 2, iter: 1500, curr loss: 0.6955273151397705, avg loss: 0.6919695310592652\n",
      "trial: 2, iter: 2000, curr loss: 0.6880757212638855, avg loss: 0.6916854759454727\n",
      "trial: 2, iter: 2500, curr loss: 0.6896840929985046, avg loss: 0.6910701359510422\n",
      "trial: 2, iter: 3000, curr loss: 0.6924358606338501, avg loss: 0.689509693145752\n",
      "trial: 2, iter: 3500, curr loss: 0.6926838755607605, avg loss: 0.6870318809747696\n",
      "trial: 2, iter: 4000, curr loss: 0.6743178367614746, avg loss: 0.6832564686536788\n",
      "trial: 2, iter: 4500, curr loss: 0.6574956774711609, avg loss: 0.6817175810337066\n",
      "trial: 2, iter: 5000, curr loss: 0.6928585767745972, avg loss: 0.6760137516260147\n",
      "trial: 2, iter: 5500, curr loss: 0.6658340692520142, avg loss: 0.67333182990551\n",
      "trial: 2, iter: 6000, curr loss: 0.656619668006897, avg loss: 0.6675401644706727\n",
      "trial: 2, ldr: -0.17978037893772125, dv: -0.53122878074646, nwj: -0.6009048223495483\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6959431171417236, avg loss: 0.6929600045681\n",
      "trial: 3, iter: 1000, curr loss: 0.6875735521316528, avg loss: 0.6925293854475021\n",
      "trial: 3, iter: 1500, curr loss: 0.6866707801818848, avg loss: 0.692017475605011\n",
      "trial: 3, iter: 2000, curr loss: 0.6852868795394897, avg loss: 0.6919790700674057\n",
      "trial: 3, iter: 2500, curr loss: 0.6988999843597412, avg loss: 0.6909365078210831\n",
      "trial: 3, iter: 3000, curr loss: 0.686652421951294, avg loss: 0.6900893154144288\n",
      "trial: 3, iter: 3500, curr loss: 0.6847759485244751, avg loss: 0.68782763838768\n",
      "trial: 3, iter: 4000, curr loss: 0.6901668310165405, avg loss: 0.6857801033258438\n",
      "trial: 3, iter: 4500, curr loss: 0.6691310405731201, avg loss: 0.6826186759471893\n",
      "trial: 3, iter: 5000, curr loss: 0.6669538617134094, avg loss: 0.6788271126747132\n",
      "trial: 3, iter: 5500, curr loss: 0.6729025840759277, avg loss: 0.6743014714717865\n",
      "trial: 3, iter: 6000, curr loss: 0.6659713983535767, avg loss: 0.6710906710624694\n",
      "trial: 3, ldr: -0.22029787302017212, dv: -0.521430253982544, nwj: -0.5716860294342041\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6936470866203308, avg loss: 0.6930064413547515\n",
      "trial: 4, iter: 1000, curr loss: 0.6893346905708313, avg loss: 0.6927223709821702\n",
      "trial: 4, iter: 1500, curr loss: 0.6900615096092224, avg loss: 0.6920789792537689\n",
      "trial: 4, iter: 2000, curr loss: 0.6819477081298828, avg loss: 0.6910327503681183\n",
      "trial: 4, iter: 2500, curr loss: 0.6631184816360474, avg loss: 0.6902125492095947\n",
      "trial: 4, iter: 3000, curr loss: 0.6768361330032349, avg loss: 0.6891604529619217\n",
      "trial: 4, iter: 3500, curr loss: 0.7246320247650146, avg loss: 0.6868015668392181\n",
      "trial: 4, iter: 4000, curr loss: 0.6723335981369019, avg loss: 0.6835411118268967\n",
      "trial: 4, iter: 4500, curr loss: 0.6577166318893433, avg loss: 0.6792572429180145\n",
      "trial: 4, iter: 5000, curr loss: 0.6590750217437744, avg loss: 0.6756333471536636\n",
      "trial: 4, iter: 5500, curr loss: 0.6992306709289551, avg loss: 0.6701377608776092\n",
      "trial: 4, iter: 6000, curr loss: 0.6757404208183289, avg loss: 0.6665466830730439\n",
      "trial: 4, ldr: -0.3257075846195221, dv: -0.8211864233016968, nwj: -0.9669915437698364\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6978725790977478, avg loss: 0.6929634078741074\n",
      "trial: 5, iter: 1000, curr loss: 0.687609076499939, avg loss: 0.6925332326889038\n",
      "trial: 5, iter: 1500, curr loss: 0.6929746866226196, avg loss: 0.6919824723005294\n",
      "trial: 5, iter: 2000, curr loss: 0.6936168074607849, avg loss: 0.6914647873640061\n",
      "trial: 5, iter: 2500, curr loss: 0.6946150064468384, avg loss: 0.6898344070911407\n",
      "trial: 5, iter: 3000, curr loss: 0.6946833729743958, avg loss: 0.6882963309288025\n",
      "trial: 5, iter: 3500, curr loss: 0.6859055757522583, avg loss: 0.6857860420942307\n",
      "trial: 5, iter: 4000, curr loss: 0.6814534664154053, avg loss: 0.682076776266098\n",
      "trial: 5, iter: 4500, curr loss: 0.6787176728248596, avg loss: 0.6771972097158432\n",
      "trial: 5, iter: 5000, curr loss: 0.7226687669754028, avg loss: 0.6719865970611573\n",
      "trial: 5, iter: 5500, curr loss: 0.68011474609375, avg loss: 0.665717845082283\n",
      "trial: 5, iter: 6000, curr loss: 0.677801251411438, avg loss: 0.661790048122406\n",
      "trial: 5, ldr: -0.2418159544467926, dv: -0.8887100219726562, nwj: -1.151416540145874\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.24939363300800324\n",
      "\tdv: -0.6798217296600342\n",
      "\tnwj: -0.7999336242675781\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6919207572937012, avg loss: 0.6932618689537048\n",
      "trial: 1, iter: 1000, curr loss: 0.6928200721740723, avg loss: 0.6924248532056808\n",
      "trial: 1, iter: 1500, curr loss: 0.6933455467224121, avg loss: 0.6924985617399215\n",
      "trial: 1, iter: 2000, curr loss: 0.6923300623893738, avg loss: 0.6921045894622803\n",
      "trial: 1, iter: 2500, curr loss: 0.6864913702011108, avg loss: 0.6905384424924851\n",
      "trial: 1, iter: 3000, curr loss: 0.6978066563606262, avg loss: 0.6902381117343903\n",
      "trial: 1, iter: 3500, curr loss: 0.6879240274429321, avg loss: 0.6884429230690002\n",
      "trial: 1, iter: 4000, curr loss: 0.7000671625137329, avg loss: 0.6856525601148605\n",
      "trial: 1, iter: 4500, curr loss: 0.6814558506011963, avg loss: 0.6812703440189362\n",
      "trial: 1, iter: 5000, curr loss: 0.6815321445465088, avg loss: 0.6785582654476165\n",
      "trial: 1, iter: 5500, curr loss: 0.662227988243103, avg loss: 0.6736994230747223\n",
      "trial: 1, iter: 6000, curr loss: 0.6209547519683838, avg loss: 0.6690939450263977\n",
      "trial: 1, ldr: -0.19195903837680817, dv: -0.6285983920097351, nwj: -0.7394568920135498\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6933305263519287, avg loss: 0.6929576297998429\n",
      "trial: 2, iter: 1000, curr loss: 0.6938010454177856, avg loss: 0.6927359131574631\n",
      "trial: 2, iter: 1500, curr loss: 0.6853553056716919, avg loss: 0.6921791719198227\n",
      "trial: 2, iter: 2000, curr loss: 0.6909662485122681, avg loss: 0.6919915362596512\n",
      "trial: 2, iter: 2500, curr loss: 0.6894516348838806, avg loss: 0.6908199470043183\n",
      "trial: 2, iter: 3000, curr loss: 0.672572135925293, avg loss: 0.6894134542942048\n",
      "trial: 2, iter: 3500, curr loss: 0.6678023338317871, avg loss: 0.6859690465927124\n",
      "trial: 2, iter: 4000, curr loss: 0.6877647042274475, avg loss: 0.6827723532915115\n",
      "trial: 2, iter: 4500, curr loss: 0.6787382364273071, avg loss: 0.6799840643405914\n",
      "trial: 2, iter: 5000, curr loss: 0.6863235831260681, avg loss: 0.6754634243249893\n",
      "trial: 2, iter: 5500, curr loss: 0.6921652555465698, avg loss: 0.6714652025699616\n",
      "trial: 2, iter: 6000, curr loss: 0.6780109405517578, avg loss: 0.666637219786644\n",
      "trial: 2, ldr: -0.17921008169651031, dv: -0.6967071294784546, nwj: -0.8570330142974854\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.695380449295044, avg loss: 0.6931453043222428\n",
      "trial: 3, iter: 1000, curr loss: 0.6960638761520386, avg loss: 0.692449718952179\n",
      "trial: 3, iter: 1500, curr loss: 0.7026060819625854, avg loss: 0.6922309811115265\n",
      "trial: 3, iter: 2000, curr loss: 0.6954010725021362, avg loss: 0.6923001991510391\n",
      "trial: 3, iter: 2500, curr loss: 0.6873147487640381, avg loss: 0.6915008223056793\n",
      "trial: 3, iter: 3000, curr loss: 0.6926796436309814, avg loss: 0.6897381910085678\n",
      "trial: 3, iter: 3500, curr loss: 0.6851800680160522, avg loss: 0.6882465173006058\n",
      "trial: 3, iter: 4000, curr loss: 0.6706571578979492, avg loss: 0.6854487339258194\n",
      "trial: 3, iter: 4500, curr loss: 0.6804631352424622, avg loss: 0.6840286757946015\n",
      "trial: 3, iter: 5000, curr loss: 0.6708040833473206, avg loss: 0.6792537947893142\n",
      "trial: 3, iter: 5500, curr loss: 0.6792678236961365, avg loss: 0.6751768788099289\n",
      "trial: 3, iter: 6000, curr loss: 0.6493279337882996, avg loss: 0.6711545784473419\n",
      "trial: 3, ldr: -0.18724504113197327, dv: -0.6002352237701416, nwj: -0.6985751390457153\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6888179183006287, avg loss: 0.6929419286251068\n",
      "trial: 4, iter: 1000, curr loss: 0.691615641117096, avg loss: 0.6926963484287262\n",
      "trial: 4, iter: 1500, curr loss: 0.6934601068496704, avg loss: 0.6922763404846192\n",
      "trial: 4, iter: 2000, curr loss: 0.6911112070083618, avg loss: 0.691886871099472\n",
      "trial: 4, iter: 2500, curr loss: 0.6975500583648682, avg loss: 0.6905324050188064\n",
      "trial: 4, iter: 3000, curr loss: 0.7036417722702026, avg loss: 0.6886033428907394\n",
      "trial: 4, iter: 3500, curr loss: 0.6659258604049683, avg loss: 0.6852424360513687\n",
      "trial: 4, iter: 4000, curr loss: 0.6751338243484497, avg loss: 0.6825317170619964\n",
      "trial: 4, iter: 4500, curr loss: 0.6891402006149292, avg loss: 0.6778293972015381\n",
      "trial: 4, iter: 5000, curr loss: 0.674699068069458, avg loss: 0.6744555934667588\n",
      "trial: 4, iter: 5500, curr loss: 0.7033114433288574, avg loss: 0.6698874143362046\n",
      "trial: 4, iter: 6000, curr loss: 0.6866028308868408, avg loss: 0.6656904232501983\n",
      "trial: 4, ldr: -0.30260324478149414, dv: -0.6604212522506714, nwj: -0.7328085899353027\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6940137147903442, avg loss: 0.6930513409376144\n",
      "trial: 5, iter: 1000, curr loss: 0.6933077573776245, avg loss: 0.692679986834526\n",
      "trial: 5, iter: 1500, curr loss: 0.6951128244400024, avg loss: 0.692166872382164\n",
      "trial: 5, iter: 2000, curr loss: 0.6834917068481445, avg loss: 0.6917009211778641\n",
      "trial: 5, iter: 2500, curr loss: 0.6865312457084656, avg loss: 0.6905479962825776\n",
      "trial: 5, iter: 3000, curr loss: 0.6992108821868896, avg loss: 0.6885622358322143\n",
      "trial: 5, iter: 3500, curr loss: 0.6956678628921509, avg loss: 0.6865166653394699\n",
      "trial: 5, iter: 4000, curr loss: 0.6760108470916748, avg loss: 0.6827454297542572\n",
      "trial: 5, iter: 4500, curr loss: 0.6302666068077087, avg loss: 0.6782364287376403\n",
      "trial: 5, iter: 5000, curr loss: 0.6760113835334778, avg loss: 0.6734554085731507\n",
      "trial: 5, iter: 5500, curr loss: 0.665668249130249, avg loss: 0.668891981601715\n",
      "trial: 5, iter: 6000, curr loss: 0.6851741075515747, avg loss: 0.6633597391843796\n",
      "trial: 5, ldr: -0.25243720412254333, dv: -0.7766658067703247, nwj: -0.9415924549102783\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.22269092202186586\n",
      "\tdv: -0.6725255608558655\n",
      "\tnwj: -0.7938932180404663\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6953380703926086, avg loss: 0.692969125509262\n",
      "trial: 1, iter: 1000, curr loss: 0.6894253492355347, avg loss: 0.6927332854270936\n",
      "trial: 1, iter: 1500, curr loss: 0.6927188038825989, avg loss: 0.692301211476326\n",
      "trial: 1, iter: 2000, curr loss: 0.6889023780822754, avg loss: 0.6915840910673141\n",
      "trial: 1, iter: 2500, curr loss: 0.6867560148239136, avg loss: 0.6913971080780029\n",
      "trial: 1, iter: 3000, curr loss: 0.67988520860672, avg loss: 0.6903092966079712\n",
      "trial: 1, iter: 3500, curr loss: 0.7076861262321472, avg loss: 0.6886135702133178\n",
      "trial: 1, iter: 4000, curr loss: 0.6881216168403625, avg loss: 0.6852823889255524\n",
      "trial: 1, iter: 4500, curr loss: 0.6819102764129639, avg loss: 0.6846394666433334\n",
      "trial: 1, iter: 5000, curr loss: 0.6800926923751831, avg loss: 0.6801249428987503\n",
      "trial: 1, iter: 5500, curr loss: 0.674950122833252, avg loss: 0.6768081243038178\n",
      "trial: 1, iter: 6000, curr loss: 0.6905995607376099, avg loss: 0.6714942362308502\n",
      "trial: 1, ldr: -0.1807677298784256, dv: -0.47597217559814453, nwj: -0.5241687297821045\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6961479187011719, avg loss: 0.6929389449357987\n",
      "trial: 2, iter: 1000, curr loss: 0.6867436170578003, avg loss: 0.6921929193735122\n",
      "trial: 2, iter: 1500, curr loss: 0.6858398914337158, avg loss: 0.6921667572259903\n",
      "trial: 2, iter: 2000, curr loss: 0.6957269906997681, avg loss: 0.6919125831127166\n",
      "trial: 2, iter: 2500, curr loss: 0.6921459436416626, avg loss: 0.6909281874895096\n",
      "trial: 2, iter: 3000, curr loss: 0.6896185874938965, avg loss: 0.6897112476825714\n",
      "trial: 2, iter: 3500, curr loss: 0.6879104375839233, avg loss: 0.687419540643692\n",
      "trial: 2, iter: 4000, curr loss: 0.6855088472366333, avg loss: 0.6846793698072433\n",
      "trial: 2, iter: 4500, curr loss: 0.6604529619216919, avg loss: 0.681096998333931\n",
      "trial: 2, iter: 5000, curr loss: 0.6867331266403198, avg loss: 0.6781142772436142\n",
      "trial: 2, iter: 5500, curr loss: 0.6822372674942017, avg loss: 0.6737739927768708\n",
      "trial: 2, iter: 6000, curr loss: 0.6554058194160461, avg loss: 0.6685067430734635\n",
      "trial: 2, ldr: -0.307943195104599, dv: -0.6545639634132385, nwj: -0.7222235202789307\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6947424411773682, avg loss: 0.6930992617607117\n",
      "trial: 3, iter: 1000, curr loss: 0.6936737298965454, avg loss: 0.6925327464342117\n",
      "trial: 3, iter: 1500, curr loss: 0.6948122978210449, avg loss: 0.6926513487100601\n",
      "trial: 3, iter: 2000, curr loss: 0.6932436227798462, avg loss: 0.6919786496162414\n",
      "trial: 3, iter: 2500, curr loss: 0.6864851713180542, avg loss: 0.6918918565511704\n",
      "trial: 3, iter: 3000, curr loss: 0.6897532939910889, avg loss: 0.6907278940677642\n",
      "trial: 3, iter: 3500, curr loss: 0.6812779307365417, avg loss: 0.689465554356575\n",
      "trial: 3, iter: 4000, curr loss: 0.6995742321014404, avg loss: 0.6871991683244705\n",
      "trial: 3, iter: 4500, curr loss: 0.6723191142082214, avg loss: 0.6854464999437332\n",
      "trial: 3, iter: 5000, curr loss: 0.6839638352394104, avg loss: 0.682016979932785\n",
      "trial: 3, iter: 5500, curr loss: 0.6612349152565002, avg loss: 0.6793711050748825\n",
      "trial: 3, iter: 6000, curr loss: 0.6720965504646301, avg loss: 0.675470342040062\n",
      "trial: 3, ldr: -0.16724391281604767, dv: -0.5306869149208069, nwj: -0.605516791343689\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6927311420440674, avg loss: 0.6930965051651001\n",
      "trial: 4, iter: 1000, curr loss: 0.6973555088043213, avg loss: 0.6924870067834854\n",
      "trial: 4, iter: 1500, curr loss: 0.6967306137084961, avg loss: 0.6919964921474456\n",
      "trial: 4, iter: 2000, curr loss: 0.6924514770507812, avg loss: 0.6909275252819062\n",
      "trial: 4, iter: 2500, curr loss: 0.6950421333312988, avg loss: 0.690063548207283\n",
      "trial: 4, iter: 3000, curr loss: 0.6894053816795349, avg loss: 0.6891948373317719\n",
      "trial: 4, iter: 3500, curr loss: 0.6792569160461426, avg loss: 0.6866724463701248\n",
      "trial: 4, iter: 4000, curr loss: 0.6806989908218384, avg loss: 0.6834206910133361\n",
      "trial: 4, iter: 4500, curr loss: 0.6584739685058594, avg loss: 0.6813771352767944\n",
      "trial: 4, iter: 5000, curr loss: 0.6871652603149414, avg loss: 0.67718115067482\n",
      "trial: 4, iter: 5500, curr loss: 0.6730612516403198, avg loss: 0.6731706006526947\n",
      "trial: 4, iter: 6000, curr loss: 0.7061200141906738, avg loss: 0.6693853698968887\n",
      "trial: 4, ldr: -0.1864146590232849, dv: -0.573587954044342, nwj: -0.6592264175415039\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6946263313293457, avg loss: 0.6932615875005722\n",
      "trial: 5, iter: 1000, curr loss: 0.6912999153137207, avg loss: 0.6924918342828751\n",
      "trial: 5, iter: 1500, curr loss: 0.6944196224212646, avg loss: 0.6921771808862686\n",
      "trial: 5, iter: 2000, curr loss: 0.6921951174736023, avg loss: 0.691750487446785\n",
      "trial: 5, iter: 2500, curr loss: 0.687250554561615, avg loss: 0.6909375952482224\n",
      "trial: 5, iter: 3000, curr loss: 0.7071079611778259, avg loss: 0.6901087124347687\n",
      "trial: 5, iter: 3500, curr loss: 0.6904382705688477, avg loss: 0.6891860848665238\n",
      "trial: 5, iter: 4000, curr loss: 0.6803175210952759, avg loss: 0.6850160641670227\n",
      "trial: 5, iter: 4500, curr loss: 0.6859893798828125, avg loss: 0.6823993364572525\n",
      "trial: 5, iter: 5000, curr loss: 0.7010128498077393, avg loss: 0.6804692674875259\n",
      "trial: 5, iter: 5500, curr loss: 0.6517545580863953, avg loss: 0.6752766481637955\n",
      "trial: 5, iter: 6000, curr loss: 0.6528006196022034, avg loss: 0.6715244035720825\n",
      "trial: 5, ldr: -0.2539372742176056, dv: -0.48735010623931885, nwj: -0.5168399810791016\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.21926135420799256\n",
      "\tdv: -0.5444322228431702\n",
      "\tnwj: -0.6055950880050659\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6948642134666443, avg loss: 0.6932409774065018\n",
      "trial: 1, iter: 1000, curr loss: 0.6925179958343506, avg loss: 0.6927417452335358\n",
      "trial: 1, iter: 1500, curr loss: 0.6920323371887207, avg loss: 0.6924132746458054\n",
      "trial: 1, iter: 2000, curr loss: 0.6924377679824829, avg loss: 0.6920538357496262\n",
      "trial: 1, iter: 2500, curr loss: 0.687579333782196, avg loss: 0.6910846197605133\n",
      "trial: 1, iter: 3000, curr loss: 0.6926360130310059, avg loss: 0.6897849063873291\n",
      "trial: 1, iter: 3500, curr loss: 0.6973493099212646, avg loss: 0.6882392364740372\n",
      "trial: 1, iter: 4000, curr loss: 0.6898819208145142, avg loss: 0.6850654269456864\n",
      "trial: 1, iter: 4500, curr loss: 0.6823976039886475, avg loss: 0.6837636877298355\n",
      "trial: 1, iter: 5000, curr loss: 0.6685398817062378, avg loss: 0.6792835240364075\n",
      "trial: 1, iter: 5500, curr loss: 0.6854037642478943, avg loss: 0.6760127403736115\n",
      "trial: 1, iter: 6000, curr loss: 0.6627020835876465, avg loss: 0.67314013671875\n",
      "trial: 1, ldr: -0.18263937532901764, dv: -0.5262818336486816, nwj: -0.5927137136459351\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6935523748397827, avg loss: 0.6930128931999207\n",
      "trial: 2, iter: 1000, curr loss: 0.6928422451019287, avg loss: 0.6926879686117172\n",
      "trial: 2, iter: 1500, curr loss: 0.686331570148468, avg loss: 0.692503852725029\n",
      "trial: 2, iter: 2000, curr loss: 0.6861984133720398, avg loss: 0.6920778406858444\n",
      "trial: 2, iter: 2500, curr loss: 0.695594310760498, avg loss: 0.6912019557952881\n",
      "trial: 2, iter: 3000, curr loss: 0.6914727687835693, avg loss: 0.6896382813453674\n",
      "trial: 2, iter: 3500, curr loss: 0.6784334778785706, avg loss: 0.6878119188547135\n",
      "trial: 2, iter: 4000, curr loss: 0.6913522481918335, avg loss: 0.6852022324800491\n",
      "trial: 2, iter: 4500, curr loss: 0.6640456914901733, avg loss: 0.6817741317749023\n",
      "trial: 2, iter: 5000, curr loss: 0.6665345430374146, avg loss: 0.6783229548931122\n",
      "trial: 2, iter: 5500, curr loss: 0.6355182528495789, avg loss: 0.6734148727655411\n",
      "trial: 2, iter: 6000, curr loss: 0.6843887567520142, avg loss: 0.6702962372303009\n",
      "trial: 2, ldr: -0.27260345220565796, dv: -0.6357277631759644, nwj: -0.7104179859161377\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.693945050239563, avg loss: 0.6932912480831146\n",
      "trial: 3, iter: 1000, curr loss: 0.6933479905128479, avg loss: 0.6928570549488068\n",
      "trial: 3, iter: 1500, curr loss: 0.6897217035293579, avg loss: 0.6926455249786377\n",
      "trial: 3, iter: 2000, curr loss: 0.6890395879745483, avg loss: 0.6920110102891922\n",
      "trial: 3, iter: 2500, curr loss: 0.6959396600723267, avg loss: 0.6911432838439941\n",
      "trial: 3, iter: 3000, curr loss: 0.6763433814048767, avg loss: 0.6901394324302673\n",
      "trial: 3, iter: 3500, curr loss: 0.6870490908622742, avg loss: 0.688206798195839\n",
      "trial: 3, iter: 4000, curr loss: 0.6859869956970215, avg loss: 0.6863621286153794\n",
      "trial: 3, iter: 4500, curr loss: 0.7201964855194092, avg loss: 0.6828448579311371\n",
      "trial: 3, iter: 5000, curr loss: 0.6755183935165405, avg loss: 0.6783094193935394\n",
      "trial: 3, iter: 5500, curr loss: 0.6735029816627502, avg loss: 0.6756767346858978\n",
      "trial: 3, iter: 6000, curr loss: 0.6671825647354126, avg loss: 0.6677455023527146\n",
      "trial: 3, ldr: -0.20858857035636902, dv: -0.6948732137680054, nwj: -0.8348513841629028\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6843452453613281, avg loss: 0.6930398799180985\n",
      "trial: 4, iter: 1000, curr loss: 0.693129301071167, avg loss: 0.6928448643684387\n",
      "trial: 4, iter: 1500, curr loss: 0.6911512613296509, avg loss: 0.6923052679300308\n",
      "trial: 4, iter: 2000, curr loss: 0.6977391839027405, avg loss: 0.6920894113779068\n",
      "trial: 4, iter: 2500, curr loss: 0.6946861743927002, avg loss: 0.6913214232921601\n",
      "trial: 4, iter: 3000, curr loss: 0.6935145258903503, avg loss: 0.6907719831466674\n",
      "trial: 4, iter: 3500, curr loss: 0.7081185579299927, avg loss: 0.6887134246826172\n",
      "trial: 4, iter: 4000, curr loss: 0.6882911920547485, avg loss: 0.6854483299255371\n",
      "trial: 4, iter: 4500, curr loss: 0.7022304534912109, avg loss: 0.6818652620315552\n",
      "trial: 4, iter: 5000, curr loss: 0.6604580879211426, avg loss: 0.6777455151081085\n",
      "trial: 4, iter: 5500, curr loss: 0.6797215938568115, avg loss: 0.6737885694503785\n",
      "trial: 4, iter: 6000, curr loss: 0.6908186674118042, avg loss: 0.6705096372365952\n",
      "trial: 4, ldr: -0.17663037776947021, dv: -0.5525875091552734, nwj: -0.6330150365829468\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6920704245567322, avg loss: 0.6930484198331833\n",
      "trial: 5, iter: 1000, curr loss: 0.6920695304870605, avg loss: 0.6927575652599335\n",
      "trial: 5, iter: 1500, curr loss: 0.6867315769195557, avg loss: 0.692424396276474\n",
      "trial: 5, iter: 2000, curr loss: 0.6936666369438171, avg loss: 0.6919822285175323\n",
      "trial: 5, iter: 2500, curr loss: 0.6969664692878723, avg loss: 0.6906198207139969\n",
      "trial: 5, iter: 3000, curr loss: 0.6754113435745239, avg loss: 0.6894979462623596\n",
      "trial: 5, iter: 3500, curr loss: 0.6724079847335815, avg loss: 0.6879829779863358\n",
      "trial: 5, iter: 4000, curr loss: 0.6821349859237671, avg loss: 0.6856214163303376\n",
      "trial: 5, iter: 4500, curr loss: 0.6767053604125977, avg loss: 0.6828133715391159\n",
      "trial: 5, iter: 5000, curr loss: 0.7219504117965698, avg loss: 0.6792315547466278\n",
      "trial: 5, iter: 5500, curr loss: 0.6878087520599365, avg loss: 0.6762337647676467\n",
      "trial: 5, iter: 6000, curr loss: 0.6681071519851685, avg loss: 0.671871974825859\n",
      "trial: 5, ldr: -0.21026785671710968, dv: -0.5109463334083557, nwj: -0.5610429048538208\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.2101459264755249\n",
      "\tdv: -0.5840833306312561\n",
      "\tnwj: -0.6664082050323487\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6905864477157593, avg loss: 0.6931020203828812\n",
      "trial: 1, iter: 1000, curr loss: 0.6951900720596313, avg loss: 0.6926908289194107\n",
      "trial: 1, iter: 1500, curr loss: 0.6958203911781311, avg loss: 0.6921321340799331\n",
      "trial: 1, iter: 2000, curr loss: 0.6858161091804504, avg loss: 0.6917366807460785\n",
      "trial: 1, iter: 2500, curr loss: 0.6934100389480591, avg loss: 0.6907019186019897\n",
      "trial: 1, iter: 3000, curr loss: 0.6849817037582397, avg loss: 0.6892433660030365\n",
      "trial: 1, iter: 3500, curr loss: 0.6866166591644287, avg loss: 0.6864446115493774\n",
      "trial: 1, iter: 4000, curr loss: 0.6811055541038513, avg loss: 0.6829746000766754\n",
      "trial: 1, iter: 4500, curr loss: 0.7019238471984863, avg loss: 0.6808799014091492\n",
      "trial: 1, iter: 5000, curr loss: 0.6889615058898926, avg loss: 0.6773625910282135\n",
      "trial: 1, iter: 5500, curr loss: 0.6640802621841431, avg loss: 0.6725107251405716\n",
      "trial: 1, iter: 6000, curr loss: 0.6510921716690063, avg loss: 0.669513445019722\n",
      "trial: 1, ldr: -0.2728905975818634, dv: -0.6297764182090759, nwj: -0.7017632722854614\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6916467547416687, avg loss: 0.6930351433753967\n",
      "trial: 2, iter: 1000, curr loss: 0.6905227899551392, avg loss: 0.692705667257309\n",
      "trial: 2, iter: 1500, curr loss: 0.694479763507843, avg loss: 0.6921951221227646\n",
      "trial: 2, iter: 2000, curr loss: 0.6991047859191895, avg loss: 0.6917623357772827\n",
      "trial: 2, iter: 2500, curr loss: 0.6861175298690796, avg loss: 0.6911418710947037\n",
      "trial: 2, iter: 3000, curr loss: 0.6795666813850403, avg loss: 0.6888356263637543\n",
      "trial: 2, iter: 3500, curr loss: 0.7089079022407532, avg loss: 0.6872674443721771\n",
      "trial: 2, iter: 4000, curr loss: 0.6891403198242188, avg loss: 0.684331192612648\n",
      "trial: 2, iter: 4500, curr loss: 0.6622939109802246, avg loss: 0.6805197540521621\n",
      "trial: 2, iter: 5000, curr loss: 0.6757620573043823, avg loss: 0.6759422832727432\n",
      "trial: 2, iter: 5500, curr loss: 0.6688143014907837, avg loss: 0.6736256736516952\n",
      "trial: 2, iter: 6000, curr loss: 0.6620578765869141, avg loss: 0.6671568570137024\n",
      "trial: 2, ldr: -0.13556507229804993, dv: -0.7503775358200073, nwj: -0.9848747253417969\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6948812007904053, avg loss: 0.6928462792634964\n",
      "trial: 3, iter: 1000, curr loss: 0.6955810189247131, avg loss: 0.6923866140842437\n",
      "trial: 3, iter: 1500, curr loss: 0.6947405338287354, avg loss: 0.6919175539016723\n",
      "trial: 3, iter: 2000, curr loss: 0.690441370010376, avg loss: 0.6910353698730469\n",
      "trial: 3, iter: 2500, curr loss: 0.690391480922699, avg loss: 0.6895036153793335\n",
      "trial: 3, iter: 3000, curr loss: 0.6686705350875854, avg loss: 0.6865322384834289\n",
      "trial: 3, iter: 3500, curr loss: 0.6688899993896484, avg loss: 0.6830610510110855\n",
      "trial: 3, iter: 4000, curr loss: 0.685763955116272, avg loss: 0.6800507384538651\n",
      "trial: 3, iter: 4500, curr loss: 0.6626374125480652, avg loss: 0.6755288605690002\n",
      "trial: 3, iter: 5000, curr loss: 0.655587375164032, avg loss: 0.6730999253988266\n",
      "trial: 3, iter: 5500, curr loss: 0.6588972806930542, avg loss: 0.6677902559041977\n",
      "trial: 3, iter: 6000, curr loss: 0.6488834619522095, avg loss: 0.6642544625997543\n",
      "trial: 3, ldr: -0.3178531527519226, dv: -0.7737971544265747, nwj: -0.8955152034759521\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.686451256275177, avg loss: 0.6932568286657333\n",
      "trial: 4, iter: 1000, curr loss: 0.6933817863464355, avg loss: 0.6925910334587098\n",
      "trial: 4, iter: 1500, curr loss: 0.691213846206665, avg loss: 0.6922887246608734\n",
      "trial: 4, iter: 2000, curr loss: 0.6910884380340576, avg loss: 0.6918257024288178\n",
      "trial: 4, iter: 2500, curr loss: 0.6957086324691772, avg loss: 0.6909088525772095\n",
      "trial: 4, iter: 3000, curr loss: 0.6838634014129639, avg loss: 0.6892932118177414\n",
      "trial: 4, iter: 3500, curr loss: 0.7068936824798584, avg loss: 0.6877357724905014\n",
      "trial: 4, iter: 4000, curr loss: 0.6709494590759277, avg loss: 0.6843726904392242\n",
      "trial: 4, iter: 4500, curr loss: 0.6809310913085938, avg loss: 0.6809428577423096\n",
      "trial: 4, iter: 5000, curr loss: 0.6624290943145752, avg loss: 0.6780065158605576\n",
      "trial: 4, iter: 5500, curr loss: 0.6808956861495972, avg loss: 0.6719015965461731\n",
      "trial: 4, iter: 6000, curr loss: 0.6663038730621338, avg loss: 0.6666808955669403\n",
      "trial: 4, ldr: -0.28007689118385315, dv: -0.5989423394203186, nwj: -0.655643105506897\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6920654773712158, avg loss: 0.6931921274662017\n",
      "trial: 5, iter: 1000, curr loss: 0.6939326524734497, avg loss: 0.6923887814283372\n",
      "trial: 5, iter: 1500, curr loss: 0.6945386528968811, avg loss: 0.6921175444126129\n",
      "trial: 5, iter: 2000, curr loss: 0.693828821182251, avg loss: 0.6919057430028915\n",
      "trial: 5, iter: 2500, curr loss: 0.6881129741668701, avg loss: 0.6906905763149261\n",
      "trial: 5, iter: 3000, curr loss: 0.6847561597824097, avg loss: 0.6889773026704789\n",
      "trial: 5, iter: 3500, curr loss: 0.6915315389633179, avg loss: 0.6876472071409225\n",
      "trial: 5, iter: 4000, curr loss: 0.6830891370773315, avg loss: 0.684423885345459\n",
      "trial: 5, iter: 4500, curr loss: 0.7003890872001648, avg loss: 0.6805719709396363\n",
      "trial: 5, iter: 5000, curr loss: 0.6948268413543701, avg loss: 0.6755562279224395\n",
      "trial: 5, iter: 5500, curr loss: 0.6816937923431396, avg loss: 0.671444167971611\n",
      "trial: 5, iter: 6000, curr loss: 0.6722348928451538, avg loss: 0.6669998482465744\n",
      "trial: 5, ldr: -0.3277706205844879, dv: -0.8377087116241455, nwj: -0.9929587841033936\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.2668312668800354\n",
      "\tdv: -0.7181204319000244\n",
      "\tnwj: -0.8461510181427002\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6922155618667603, avg loss: 0.6930057473182678\n",
      "trial: 1, iter: 1000, curr loss: 0.6987588405609131, avg loss: 0.69248998939991\n",
      "trial: 1, iter: 1500, curr loss: 0.6852563619613647, avg loss: 0.6922606105804443\n",
      "trial: 1, iter: 2000, curr loss: 0.6863491535186768, avg loss: 0.6919667831659317\n",
      "trial: 1, iter: 2500, curr loss: 0.6917473077774048, avg loss: 0.6909563392400742\n",
      "trial: 1, iter: 3000, curr loss: 0.6898118257522583, avg loss: 0.6896271764039993\n",
      "trial: 1, iter: 3500, curr loss: 0.6890808343887329, avg loss: 0.6874974789619446\n",
      "trial: 1, iter: 4000, curr loss: 0.6752150058746338, avg loss: 0.6845914611816406\n",
      "trial: 1, iter: 4500, curr loss: 0.6680707335472107, avg loss: 0.6801650077104568\n",
      "trial: 1, iter: 5000, curr loss: 0.6492215394973755, avg loss: 0.675991226196289\n",
      "trial: 1, iter: 5500, curr loss: 0.6636033058166504, avg loss: 0.6744599081277848\n",
      "trial: 1, iter: 6000, curr loss: 0.6326745748519897, avg loss: 0.6681817100048065\n",
      "trial: 1, ldr: -0.29250243306159973, dv: -0.5804954767227173, nwj: -0.6262503862380981\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6935430765151978, avg loss: 0.6930491480827331\n",
      "trial: 2, iter: 1000, curr loss: 0.6940173506736755, avg loss: 0.6927805655002593\n",
      "trial: 2, iter: 1500, curr loss: 0.6901566386222839, avg loss: 0.6923121209144593\n",
      "trial: 2, iter: 2000, curr loss: 0.6937251687049866, avg loss: 0.6916839035749436\n",
      "trial: 2, iter: 2500, curr loss: 0.6836585998535156, avg loss: 0.6908785381317138\n",
      "trial: 2, iter: 3000, curr loss: 0.6876220703125, avg loss: 0.6892663967609406\n",
      "trial: 2, iter: 3500, curr loss: 0.690478503704071, avg loss: 0.6870471661090851\n",
      "trial: 2, iter: 4000, curr loss: 0.6830508708953857, avg loss: 0.6837946629524231\n",
      "trial: 2, iter: 4500, curr loss: 0.6474424600601196, avg loss: 0.6795101194381714\n",
      "trial: 2, iter: 5000, curr loss: 0.7143659591674805, avg loss: 0.6760445420742035\n",
      "trial: 2, iter: 5500, curr loss: 0.6937004327774048, avg loss: 0.671257016658783\n",
      "trial: 2, iter: 6000, curr loss: 0.6408644914627075, avg loss: 0.6690311275720596\n",
      "trial: 2, ldr: -0.2255726307630539, dv: -0.7642953395843506, nwj: -0.9393889904022217\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6904250383377075, avg loss: 0.693071998000145\n",
      "trial: 3, iter: 1000, curr loss: 0.6842882037162781, avg loss: 0.692432893872261\n",
      "trial: 3, iter: 1500, curr loss: 0.6926664113998413, avg loss: 0.6918327467441558\n",
      "trial: 3, iter: 2000, curr loss: 0.6903491616249084, avg loss: 0.6911336507797241\n",
      "trial: 3, iter: 2500, curr loss: 0.6905293464660645, avg loss: 0.6900413479804993\n",
      "trial: 3, iter: 3000, curr loss: 0.6809554100036621, avg loss: 0.687381728887558\n",
      "trial: 3, iter: 3500, curr loss: 0.6817559003829956, avg loss: 0.6851248430013657\n",
      "trial: 3, iter: 4000, curr loss: 0.6637288331985474, avg loss: 0.6812025458812714\n",
      "trial: 3, iter: 4500, curr loss: 0.6841316223144531, avg loss: 0.6779986922740936\n",
      "trial: 3, iter: 5000, curr loss: 0.6834874749183655, avg loss: 0.6733571903705597\n",
      "trial: 3, iter: 5500, curr loss: 0.6289982795715332, avg loss: 0.6690356483459473\n",
      "trial: 3, iter: 6000, curr loss: 0.6736348867416382, avg loss: 0.6643372058868409\n",
      "trial: 3, ldr: -0.19635210931301117, dv: -3.5641086101531982, nwj: -28.209720611572266\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6905686855316162, avg loss: 0.6931633793115616\n",
      "trial: 4, iter: 1000, curr loss: 0.6983082294464111, avg loss: 0.69261288356781\n",
      "trial: 4, iter: 1500, curr loss: 0.6931458711624146, avg loss: 0.6921515756845474\n",
      "trial: 4, iter: 2000, curr loss: 0.6859760284423828, avg loss: 0.6918705526590347\n",
      "trial: 4, iter: 2500, curr loss: 0.6920444369316101, avg loss: 0.6913265148401261\n",
      "trial: 4, iter: 3000, curr loss: 0.6849668622016907, avg loss: 0.6901624443531036\n",
      "trial: 4, iter: 3500, curr loss: 0.6808902621269226, avg loss: 0.6887437070608139\n",
      "trial: 4, iter: 4000, curr loss: 0.6803423166275024, avg loss: 0.6857505285739899\n",
      "trial: 4, iter: 4500, curr loss: 0.6618881225585938, avg loss: 0.68098530960083\n",
      "trial: 4, iter: 5000, curr loss: 0.6705472469329834, avg loss: 0.6761943966150283\n",
      "trial: 4, iter: 5500, curr loss: 0.7025147676467896, avg loss: 0.6730362740755081\n",
      "trial: 4, iter: 6000, curr loss: 0.6264011859893799, avg loss: 0.667352618932724\n",
      "trial: 4, ldr: -0.2170671671628952, dv: -0.642948567867279, nwj: -0.7480063438415527\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6884476542472839, avg loss: 0.6929119492769241\n",
      "trial: 5, iter: 1000, curr loss: 0.6967665553092957, avg loss: 0.6928161019086838\n",
      "trial: 5, iter: 1500, curr loss: 0.6921430826187134, avg loss: 0.6925705531835556\n",
      "trial: 5, iter: 2000, curr loss: 0.6908196806907654, avg loss: 0.6920340656042099\n",
      "trial: 5, iter: 2500, curr loss: 0.6863881349563599, avg loss: 0.6913279031515122\n",
      "trial: 5, iter: 3000, curr loss: 0.6920228004455566, avg loss: 0.690582741856575\n",
      "trial: 5, iter: 3500, curr loss: 0.685070276260376, avg loss: 0.688742577791214\n",
      "trial: 5, iter: 4000, curr loss: 0.685334324836731, avg loss: 0.6853742388486862\n",
      "trial: 5, iter: 4500, curr loss: 0.6779623031616211, avg loss: 0.6818351427316666\n",
      "trial: 5, iter: 5000, curr loss: 0.6585977673530579, avg loss: 0.6776007120609283\n",
      "trial: 5, iter: 5500, curr loss: 0.6431580781936646, avg loss: 0.6741027073860169\n",
      "trial: 5, iter: 6000, curr loss: 0.6642166376113892, avg loss: 0.6706479332447052\n",
      "trial: 5, ldr: -0.17069830000400543, dv: -0.637432336807251, nwj: -0.7654755115509033\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.22043852806091307\n",
      "\tdv: -1.2378560662269593\n",
      "\tnwj: -6.257768368721008\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6970013380050659, avg loss: 0.6932188725471496\n",
      "trial: 1, iter: 1000, curr loss: 0.6923761367797852, avg loss: 0.6927409139871598\n",
      "trial: 1, iter: 1500, curr loss: 0.6939576268196106, avg loss: 0.692447334766388\n",
      "trial: 1, iter: 2000, curr loss: 0.6943191289901733, avg loss: 0.6922223101854325\n",
      "trial: 1, iter: 2500, curr loss: 0.6875548958778381, avg loss: 0.6919219117164612\n",
      "trial: 1, iter: 3000, curr loss: 0.6924612522125244, avg loss: 0.6913367823362351\n",
      "trial: 1, iter: 3500, curr loss: 0.6888483166694641, avg loss: 0.6898894622325897\n",
      "trial: 1, iter: 4000, curr loss: 0.6856964826583862, avg loss: 0.6868332817554473\n",
      "trial: 1, iter: 4500, curr loss: 0.6962913274765015, avg loss: 0.6835566620826721\n",
      "trial: 1, iter: 5000, curr loss: 0.6939135789871216, avg loss: 0.6789072965383529\n",
      "trial: 1, iter: 5500, curr loss: 0.6501092314720154, avg loss: 0.6746751652956009\n",
      "trial: 1, iter: 6000, curr loss: 0.6573737859725952, avg loss: 0.6692905713319779\n",
      "trial: 1, ldr: -0.23811988532543182, dv: -0.7388381361961365, nwj: -0.8880257606506348\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6939331293106079, avg loss: 0.6929757425785065\n",
      "trial: 2, iter: 1000, curr loss: 0.6902598142623901, avg loss: 0.6925232055187225\n",
      "trial: 2, iter: 1500, curr loss: 0.6925534605979919, avg loss: 0.6921399722099304\n",
      "trial: 2, iter: 2000, curr loss: 0.6913857460021973, avg loss: 0.6918662066459655\n",
      "trial: 2, iter: 2500, curr loss: 0.6899400949478149, avg loss: 0.6908023897409439\n",
      "trial: 2, iter: 3000, curr loss: 0.6826003789901733, avg loss: 0.6892313470840454\n",
      "trial: 2, iter: 3500, curr loss: 0.6822956800460815, avg loss: 0.6856677516698837\n",
      "trial: 2, iter: 4000, curr loss: 0.6814202070236206, avg loss: 0.6828773124217987\n",
      "trial: 2, iter: 4500, curr loss: 0.6416698098182678, avg loss: 0.6785361059904098\n",
      "trial: 2, iter: 5000, curr loss: 0.6571573615074158, avg loss: 0.6731555613279343\n",
      "trial: 2, iter: 5500, curr loss: 0.6555685997009277, avg loss: 0.67093822991848\n",
      "trial: 2, iter: 6000, curr loss: 0.6475627422332764, avg loss: 0.6645320227146149\n",
      "trial: 2, ldr: -0.24753953516483307, dv: -0.8360868096351624, nwj: -1.0489091873168945\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6967097520828247, avg loss: 0.6931298565864563\n",
      "trial: 3, iter: 1000, curr loss: 0.6942516565322876, avg loss: 0.6926878025531769\n",
      "trial: 3, iter: 1500, curr loss: 0.6934303045272827, avg loss: 0.6925190806388855\n",
      "trial: 3, iter: 2000, curr loss: 0.6843248605728149, avg loss: 0.691977412700653\n",
      "trial: 3, iter: 2500, curr loss: 0.696937084197998, avg loss: 0.6906714514493942\n",
      "trial: 3, iter: 3000, curr loss: 0.695393443107605, avg loss: 0.6891563351154327\n",
      "trial: 3, iter: 3500, curr loss: 0.6700254678726196, avg loss: 0.6870498230457306\n",
      "trial: 3, iter: 4000, curr loss: 0.6863369941711426, avg loss: 0.6835287058353424\n",
      "trial: 3, iter: 4500, curr loss: 0.6776915192604065, avg loss: 0.6814461452960968\n",
      "trial: 3, iter: 5000, curr loss: 0.6522461771965027, avg loss: 0.6767105958461761\n",
      "trial: 3, iter: 5500, curr loss: 0.6649726629257202, avg loss: 0.6708054070472718\n",
      "trial: 3, iter: 6000, curr loss: 0.6657211780548096, avg loss: 0.6672691506147385\n",
      "trial: 3, ldr: -0.28090646862983704, dv: -0.7081919312477112, nwj: -0.8139966726303101\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6930927038192749, avg loss: 0.6932367364168167\n",
      "trial: 4, iter: 1000, curr loss: 0.6930280923843384, avg loss: 0.692776763677597\n",
      "trial: 4, iter: 1500, curr loss: 0.6948220133781433, avg loss: 0.692173881649971\n",
      "trial: 4, iter: 2000, curr loss: 0.697411060333252, avg loss: 0.6913581855297088\n",
      "trial: 4, iter: 2500, curr loss: 0.6903996467590332, avg loss: 0.6896308780908584\n",
      "trial: 4, iter: 3000, curr loss: 0.6817657947540283, avg loss: 0.6880464795827865\n",
      "trial: 4, iter: 3500, curr loss: 0.6896620988845825, avg loss: 0.6861063551902771\n",
      "trial: 4, iter: 4000, curr loss: 0.6759549379348755, avg loss: 0.6827686361074448\n",
      "trial: 4, iter: 4500, curr loss: 0.670734167098999, avg loss: 0.6797699347734452\n",
      "trial: 4, iter: 5000, curr loss: 0.6673258543014526, avg loss: 0.6760927881002426\n",
      "trial: 4, iter: 5500, curr loss: 0.6692195534706116, avg loss: 0.6725627889633179\n",
      "trial: 4, iter: 6000, curr loss: 0.700459361076355, avg loss: 0.6658734647035599\n",
      "trial: 4, ldr: -0.2811795771121979, dv: -0.7677496671676636, nwj: -0.9079066514968872\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6865887641906738, avg loss: 0.6931924587488174\n",
      "trial: 5, iter: 1000, curr loss: 0.6946874260902405, avg loss: 0.6925542267560959\n",
      "trial: 5, iter: 1500, curr loss: 0.6989277601242065, avg loss: 0.6923551089763641\n",
      "trial: 5, iter: 2000, curr loss: 0.684960126876831, avg loss: 0.6919208700656891\n",
      "trial: 5, iter: 2500, curr loss: 0.6845428943634033, avg loss: 0.6908622697591782\n",
      "trial: 5, iter: 3000, curr loss: 0.6687407493591309, avg loss: 0.6888004863262176\n",
      "trial: 5, iter: 3500, curr loss: 0.6841710209846497, avg loss: 0.6869940255880356\n",
      "trial: 5, iter: 4000, curr loss: 0.6854062080383301, avg loss: 0.6845790282487869\n",
      "trial: 5, iter: 4500, curr loss: 0.6966177225112915, avg loss: 0.6813172973394394\n",
      "trial: 5, iter: 5000, curr loss: 0.6805869936943054, avg loss: 0.676024084687233\n",
      "trial: 5, iter: 5500, curr loss: 0.705256462097168, avg loss: 0.6720456035137177\n",
      "trial: 5, iter: 6000, curr loss: 0.6678269505500793, avg loss: 0.6676115599870682\n",
      "trial: 5, ldr: -0.28957194089889526, dv: -0.6083637475967407, nwj: -0.6650369167327881\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.267463481426239\n",
      "\tdv: -0.7318460583686829\n",
      "\tnwj: -0.8647750377655029\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6926388740539551, avg loss: 0.6932111562490463\n",
      "trial: 1, iter: 1000, curr loss: 0.6922488212585449, avg loss: 0.6927316694259643\n",
      "trial: 1, iter: 1500, curr loss: 0.6791569590568542, avg loss: 0.6919174885749817\n",
      "trial: 1, iter: 2000, curr loss: 0.693415641784668, avg loss: 0.6918346796035767\n",
      "trial: 1, iter: 2500, curr loss: 0.6808335185050964, avg loss: 0.6904267214536667\n",
      "trial: 1, iter: 3000, curr loss: 0.6780303716659546, avg loss: 0.688680822968483\n",
      "trial: 1, iter: 3500, curr loss: 0.6648157835006714, avg loss: 0.6851558798551559\n",
      "trial: 1, iter: 4000, curr loss: 0.6776903867721558, avg loss: 0.6809428578615189\n",
      "trial: 1, iter: 4500, curr loss: 0.6537640690803528, avg loss: 0.6764035764932632\n",
      "trial: 1, iter: 5000, curr loss: 0.6729528903961182, avg loss: 0.672051035284996\n",
      "trial: 1, iter: 5500, curr loss: 0.652572751045227, avg loss: 0.6681294733285904\n",
      "trial: 1, iter: 6000, curr loss: 0.6499515771865845, avg loss: 0.6640228065252304\n",
      "trial: 1, ldr: -0.2666321098804474, dv: -0.900209903717041, nwj: -1.1509723663330078\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6909846067428589, avg loss: 0.6932071596384048\n",
      "trial: 2, iter: 1000, curr loss: 0.6883323192596436, avg loss: 0.6926477621793747\n",
      "trial: 2, iter: 1500, curr loss: 0.6886204481124878, avg loss: 0.6925384291410446\n",
      "trial: 2, iter: 2000, curr loss: 0.6902511715888977, avg loss: 0.6913397016525269\n",
      "trial: 2, iter: 2500, curr loss: 0.6756892204284668, avg loss: 0.6903204171657562\n",
      "trial: 2, iter: 3000, curr loss: 0.6909221410751343, avg loss: 0.6877422974109649\n",
      "trial: 2, iter: 3500, curr loss: 0.6852187514305115, avg loss: 0.6845130676031113\n",
      "trial: 2, iter: 4000, curr loss: 0.6935063004493713, avg loss: 0.6819023009538651\n",
      "trial: 2, iter: 4500, curr loss: 0.6643788814544678, avg loss: 0.6771522983312607\n",
      "trial: 2, iter: 5000, curr loss: 0.6632159352302551, avg loss: 0.6752922024726867\n",
      "trial: 2, iter: 5500, curr loss: 0.6422290205955505, avg loss: 0.6707732144594193\n",
      "trial: 2, iter: 6000, curr loss: 0.6393678188323975, avg loss: 0.6673849782943726\n",
      "trial: 2, ldr: -0.16184799373149872, dv: -0.6316778063774109, nwj: -0.7615698575973511\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.685810923576355, avg loss: 0.6930520950555802\n",
      "trial: 3, iter: 1000, curr loss: 0.6927704811096191, avg loss: 0.6926261471509934\n",
      "trial: 3, iter: 1500, curr loss: 0.6941236257553101, avg loss: 0.6923311669826507\n",
      "trial: 3, iter: 2000, curr loss: 0.6938028335571289, avg loss: 0.6921082597970962\n",
      "trial: 3, iter: 2500, curr loss: 0.7046530246734619, avg loss: 0.6912671196460723\n",
      "trial: 3, iter: 3000, curr loss: 0.6878924369812012, avg loss: 0.6899048330783843\n",
      "trial: 3, iter: 3500, curr loss: 0.6886309385299683, avg loss: 0.6870218786001205\n",
      "trial: 3, iter: 4000, curr loss: 0.7147988080978394, avg loss: 0.6839903573989868\n",
      "trial: 3, iter: 4500, curr loss: 0.6794621348381042, avg loss: 0.6789515045881271\n",
      "trial: 3, iter: 5000, curr loss: 0.6810970902442932, avg loss: 0.6753484898805618\n",
      "trial: 3, iter: 5500, curr loss: 0.7101888060569763, avg loss: 0.6702672395706176\n",
      "trial: 3, iter: 6000, curr loss: 0.720169186592102, avg loss: 0.6671362804174423\n",
      "trial: 3, ldr: -0.26837074756622314, dv: -0.7678232192993164, nwj: -0.9161895513534546\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6872451901435852, avg loss: 0.6931072824001312\n",
      "trial: 4, iter: 1000, curr loss: 0.6936773061752319, avg loss: 0.6927714499235154\n",
      "trial: 4, iter: 1500, curr loss: 0.6960749626159668, avg loss: 0.6924971256256104\n",
      "trial: 4, iter: 2000, curr loss: 0.6944872140884399, avg loss: 0.6924545156955719\n",
      "trial: 4, iter: 2500, curr loss: 0.6834146976470947, avg loss: 0.6919249044656753\n",
      "trial: 4, iter: 3000, curr loss: 0.6854986548423767, avg loss: 0.6906390520334243\n",
      "trial: 4, iter: 3500, curr loss: 0.6854699850082397, avg loss: 0.6883207597732544\n",
      "trial: 4, iter: 4000, curr loss: 0.69322669506073, avg loss: 0.6864553875923157\n",
      "trial: 4, iter: 4500, curr loss: 0.6827130913734436, avg loss: 0.6830282560586929\n",
      "trial: 4, iter: 5000, curr loss: 0.6720539927482605, avg loss: 0.6796540051698685\n",
      "trial: 4, iter: 5500, curr loss: 0.670360803604126, avg loss: 0.6745080817937851\n",
      "trial: 4, iter: 6000, curr loss: 0.6925875544548035, avg loss: 0.6698473390340806\n",
      "trial: 4, ldr: -0.1751766800880432, dv: -0.4701074957847595, nwj: -0.5182101726531982\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.686809778213501, avg loss: 0.6931195954084396\n",
      "trial: 5, iter: 1000, curr loss: 0.6913206577301025, avg loss: 0.6925555100440979\n",
      "trial: 5, iter: 1500, curr loss: 0.6905527114868164, avg loss: 0.6922839105129241\n",
      "trial: 5, iter: 2000, curr loss: 0.689614474773407, avg loss: 0.6916425681114197\n",
      "trial: 5, iter: 2500, curr loss: 0.6977943181991577, avg loss: 0.6900246205329895\n",
      "trial: 5, iter: 3000, curr loss: 0.6701974868774414, avg loss: 0.6881501457691193\n",
      "trial: 5, iter: 3500, curr loss: 0.6880303621292114, avg loss: 0.6864484384059906\n",
      "trial: 5, iter: 4000, curr loss: 0.7129536271095276, avg loss: 0.68195339345932\n",
      "trial: 5, iter: 4500, curr loss: 0.6778128743171692, avg loss: 0.6786682775020599\n",
      "trial: 5, iter: 5000, curr loss: 0.6641431450843811, avg loss: 0.67285591340065\n",
      "trial: 5, iter: 5500, curr loss: 0.688924252986908, avg loss: 0.6685135794878005\n",
      "trial: 5, iter: 6000, curr loss: 0.6379910707473755, avg loss: 0.6628208539485931\n",
      "trial: 5, ldr: -0.24694867432117462, dv: -0.9645199775695801, nwj: -1.296398401260376\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.22379524111747742\n",
      "\tdv: -0.7468676805496216\n",
      "\tnwj: -0.9286680698394776\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6912460923194885, avg loss: 0.6932164483070373\n",
      "trial: 1, iter: 1000, curr loss: 0.6915020942687988, avg loss: 0.6928129929304123\n",
      "trial: 1, iter: 1500, curr loss: 0.69296795129776, avg loss: 0.6920238871574402\n",
      "trial: 1, iter: 2000, curr loss: 0.6952049136161804, avg loss: 0.6915717483758926\n",
      "trial: 1, iter: 2500, curr loss: 0.6800827383995056, avg loss: 0.6907960461378098\n",
      "trial: 1, iter: 3000, curr loss: 0.6899687051773071, avg loss: 0.6904881964921952\n",
      "trial: 1, iter: 3500, curr loss: 0.6850871443748474, avg loss: 0.6878176617622376\n",
      "trial: 1, iter: 4000, curr loss: 0.6748273372650146, avg loss: 0.6858365641832351\n",
      "trial: 1, iter: 4500, curr loss: 0.701483428478241, avg loss: 0.682284795999527\n",
      "trial: 1, iter: 5000, curr loss: 0.7008638381958008, avg loss: 0.678726622223854\n",
      "trial: 1, iter: 5500, curr loss: 0.6616789698600769, avg loss: 0.6742112375497818\n",
      "trial: 1, iter: 6000, curr loss: 0.6977276802062988, avg loss: 0.6706293963193893\n",
      "trial: 1, ldr: -0.20073476433753967, dv: -0.48334357142448425, nwj: -0.5273208618164062\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.695236325263977, avg loss: 0.6929547382593155\n",
      "trial: 2, iter: 1000, curr loss: 0.6880202293395996, avg loss: 0.6925008100271225\n",
      "trial: 2, iter: 1500, curr loss: 0.6940706968307495, avg loss: 0.6920386000871658\n",
      "trial: 2, iter: 2000, curr loss: 0.6858631372451782, avg loss: 0.6914720460176468\n",
      "trial: 2, iter: 2500, curr loss: 0.7093154191970825, avg loss: 0.6906217379570008\n",
      "trial: 2, iter: 3000, curr loss: 0.6775697469711304, avg loss: 0.6888410600423813\n",
      "trial: 2, iter: 3500, curr loss: 0.6740650534629822, avg loss: 0.6870861856937408\n",
      "trial: 2, iter: 4000, curr loss: 0.6887735724449158, avg loss: 0.6842545731067657\n",
      "trial: 2, iter: 4500, curr loss: 0.684683084487915, avg loss: 0.6795693023204803\n",
      "trial: 2, iter: 5000, curr loss: 0.677761435508728, avg loss: 0.6757095137834549\n",
      "trial: 2, iter: 5500, curr loss: 0.6611389517784119, avg loss: 0.6700183460712433\n",
      "trial: 2, iter: 6000, curr loss: 0.6812126636505127, avg loss: 0.6665206427574157\n",
      "trial: 2, ldr: -0.23627826571464539, dv: -0.6659568548202515, nwj: -0.773041844367981\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6937147378921509, avg loss: 0.6931205179691314\n",
      "trial: 3, iter: 1000, curr loss: 0.6960538029670715, avg loss: 0.6925111886262894\n",
      "trial: 3, iter: 1500, curr loss: 0.6928914785385132, avg loss: 0.6923649727106095\n",
      "trial: 3, iter: 2000, curr loss: 0.6940526366233826, avg loss: 0.6920030982494354\n",
      "trial: 3, iter: 2500, curr loss: 0.6922244429588318, avg loss: 0.6913491448163986\n",
      "trial: 3, iter: 3000, curr loss: 0.671337902545929, avg loss: 0.6901344683170318\n",
      "trial: 3, iter: 3500, curr loss: 0.6580324769020081, avg loss: 0.6882889170646668\n",
      "trial: 3, iter: 4000, curr loss: 0.6871294379234314, avg loss: 0.6861774233579636\n",
      "trial: 3, iter: 4500, curr loss: 0.6627926826477051, avg loss: 0.6831101977825165\n",
      "trial: 3, iter: 5000, curr loss: 0.6794625520706177, avg loss: 0.6799297308921814\n",
      "trial: 3, iter: 5500, curr loss: 0.6566963195800781, avg loss: 0.6758944537639618\n",
      "trial: 3, iter: 6000, curr loss: 0.6540969610214233, avg loss: 0.6713345800638199\n",
      "trial: 3, ldr: -0.1883639097213745, dv: -0.5662355422973633, nwj: -0.647539496421814\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6984294056892395, avg loss: 0.6931597547531128\n",
      "trial: 4, iter: 1000, curr loss: 0.6945107579231262, avg loss: 0.6926819062232972\n",
      "trial: 4, iter: 1500, curr loss: 0.6909259557723999, avg loss: 0.6922922931909561\n",
      "trial: 4, iter: 2000, curr loss: 0.684817910194397, avg loss: 0.6917245069742203\n",
      "trial: 4, iter: 2500, curr loss: 0.6984875202178955, avg loss: 0.6903300077915192\n",
      "trial: 4, iter: 3000, curr loss: 0.6734490990638733, avg loss: 0.6890392626523971\n",
      "trial: 4, iter: 3500, curr loss: 0.6769630908966064, avg loss: 0.6867632653713226\n",
      "trial: 4, iter: 4000, curr loss: 0.6798666715621948, avg loss: 0.6842949841022491\n",
      "trial: 4, iter: 4500, curr loss: 0.6658076047897339, avg loss: 0.6812052992582321\n",
      "trial: 4, iter: 5000, curr loss: 0.6781315803527832, avg loss: 0.6779496804475784\n",
      "trial: 4, iter: 5500, curr loss: 0.6567920446395874, avg loss: 0.6761991537809372\n",
      "trial: 4, iter: 6000, curr loss: 0.6524641513824463, avg loss: 0.6696195214986801\n",
      "trial: 4, ldr: -0.2398832142353058, dv: -0.6006700992584229, nwj: -0.6743408441543579\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6913579702377319, avg loss: 0.693029066324234\n",
      "trial: 5, iter: 1000, curr loss: 0.6923804879188538, avg loss: 0.6926943619251251\n",
      "trial: 5, iter: 1500, curr loss: 0.7044233083724976, avg loss: 0.6923122612237931\n",
      "trial: 5, iter: 2000, curr loss: 0.6797801852226257, avg loss: 0.6917755811214447\n",
      "trial: 5, iter: 2500, curr loss: 0.6936599016189575, avg loss: 0.6907444230318069\n",
      "trial: 5, iter: 3000, curr loss: 0.6929839253425598, avg loss: 0.6891362881660461\n",
      "trial: 5, iter: 3500, curr loss: 0.6804776191711426, avg loss: 0.6867820883989334\n",
      "trial: 5, iter: 4000, curr loss: 0.6945427060127258, avg loss: 0.6841706731319428\n",
      "trial: 5, iter: 4500, curr loss: 0.6863337159156799, avg loss: 0.6797461910247803\n",
      "trial: 5, iter: 5000, curr loss: 0.6572657823562622, avg loss: 0.6775646934509277\n",
      "trial: 5, iter: 5500, curr loss: 0.6717456579208374, avg loss: 0.674175730586052\n",
      "trial: 5, iter: 6000, curr loss: 0.6488211154937744, avg loss: 0.6699941843748093\n",
      "trial: 5, ldr: -0.2189052402973175, dv: -0.652894139289856, nwj: -0.7623069286346436\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.21683307886123657\n",
      "\tdv: -0.5938200414180755\n",
      "\tnwj: -0.6769099950790405\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6960330009460449, avg loss: 0.6930840553045273\n",
      "trial: 1, iter: 1000, curr loss: 0.6898629665374756, avg loss: 0.692425191283226\n",
      "trial: 1, iter: 1500, curr loss: 0.688841700553894, avg loss: 0.6923423811197281\n",
      "trial: 1, iter: 2000, curr loss: 0.6919736862182617, avg loss: 0.6917920368909836\n",
      "trial: 1, iter: 2500, curr loss: 0.681167721748352, avg loss: 0.6908113008737564\n",
      "trial: 1, iter: 3000, curr loss: 0.6867589950561523, avg loss: 0.6893670378923417\n",
      "trial: 1, iter: 3500, curr loss: 0.6766119003295898, avg loss: 0.6874021891355514\n",
      "trial: 1, iter: 4000, curr loss: 0.6813274621963501, avg loss: 0.6850881983041763\n",
      "trial: 1, iter: 4500, curr loss: 0.6813448667526245, avg loss: 0.6821811052560807\n",
      "trial: 1, iter: 5000, curr loss: 0.6840518116950989, avg loss: 0.678049830198288\n",
      "trial: 1, iter: 5500, curr loss: 0.6770248413085938, avg loss: 0.6746727231740951\n",
      "trial: 1, iter: 6000, curr loss: 0.7219317555427551, avg loss: 0.6687552109956741\n",
      "trial: 1, ldr: -0.18555760383605957, dv: -0.6392183303833008, nwj: -0.7596215009689331\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6903098821640015, avg loss: 0.693255607843399\n",
      "trial: 2, iter: 1000, curr loss: 0.6896299123764038, avg loss: 0.6925068253278732\n",
      "trial: 2, iter: 1500, curr loss: 0.693374514579773, avg loss: 0.6924942317008972\n",
      "trial: 2, iter: 2000, curr loss: 0.6997712850570679, avg loss: 0.6917550518512726\n",
      "trial: 2, iter: 2500, curr loss: 0.6903945207595825, avg loss: 0.6906279743909836\n",
      "trial: 2, iter: 3000, curr loss: 0.6852684617042542, avg loss: 0.6888261022567749\n",
      "trial: 2, iter: 3500, curr loss: 0.6702680587768555, avg loss: 0.686716789841652\n",
      "trial: 2, iter: 4000, curr loss: 0.6832119226455688, avg loss: 0.6855613572597503\n",
      "trial: 2, iter: 4500, curr loss: 0.6723458766937256, avg loss: 0.6816532975435257\n",
      "trial: 2, iter: 5000, curr loss: 0.6400206089019775, avg loss: 0.6782779928445816\n",
      "trial: 2, iter: 5500, curr loss: 0.6342724561691284, avg loss: 0.6751416244506836\n",
      "trial: 2, iter: 6000, curr loss: 0.6522119045257568, avg loss: 0.6709653861522674\n",
      "trial: 2, ldr: -0.29214638471603394, dv: -0.563005805015564, nwj: -0.6032371520996094\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6922783851623535, avg loss: 0.693152260184288\n",
      "trial: 3, iter: 1000, curr loss: 0.6963566541671753, avg loss: 0.692459644317627\n",
      "trial: 3, iter: 1500, curr loss: 0.6867173910140991, avg loss: 0.6920583982467652\n",
      "trial: 3, iter: 2000, curr loss: 0.6836435198783875, avg loss: 0.6908897027969361\n",
      "trial: 3, iter: 2500, curr loss: 0.6743874549865723, avg loss: 0.690357515335083\n",
      "trial: 3, iter: 3000, curr loss: 0.6974799036979675, avg loss: 0.6885260791778565\n",
      "trial: 3, iter: 3500, curr loss: 0.7113186717033386, avg loss: 0.686681032538414\n",
      "trial: 3, iter: 4000, curr loss: 0.6573531031608582, avg loss: 0.6844170053005219\n",
      "trial: 3, iter: 4500, curr loss: 0.6824166178703308, avg loss: 0.6811669167280198\n",
      "trial: 3, iter: 5000, curr loss: 0.6972836256027222, avg loss: 0.6781736376285553\n",
      "trial: 3, iter: 5500, curr loss: 0.6814284324645996, avg loss: 0.6729914368391037\n",
      "trial: 3, iter: 6000, curr loss: 0.6591991186141968, avg loss: 0.6701338911056518\n",
      "trial: 3, ldr: -0.19316989183425903, dv: -0.6966261267662048, nwj: -0.8475995063781738\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6937284469604492, avg loss: 0.6930030668973923\n",
      "trial: 4, iter: 1000, curr loss: 0.693467378616333, avg loss: 0.6925363920927048\n",
      "trial: 4, iter: 1500, curr loss: 0.6843814253807068, avg loss: 0.6921640002727508\n",
      "trial: 4, iter: 2000, curr loss: 0.696302056312561, avg loss: 0.691685830950737\n",
      "trial: 4, iter: 2500, curr loss: 0.689156711101532, avg loss: 0.6907870138883591\n",
      "trial: 4, iter: 3000, curr loss: 0.6999391913414001, avg loss: 0.6895284640789032\n",
      "trial: 4, iter: 3500, curr loss: 0.6739896535873413, avg loss: 0.6882333220243454\n",
      "trial: 4, iter: 4000, curr loss: 0.6753700971603394, avg loss: 0.6848380045890808\n",
      "trial: 4, iter: 4500, curr loss: 0.714587926864624, avg loss: 0.6822002905607224\n",
      "trial: 4, iter: 5000, curr loss: 0.6716766953468323, avg loss: 0.6790651270151138\n",
      "trial: 4, iter: 5500, curr loss: 0.663151204586029, avg loss: 0.6757016096115113\n",
      "trial: 4, iter: 6000, curr loss: 0.6640243530273438, avg loss: 0.6691503174304962\n",
      "trial: 4, ldr: -0.2116328924894333, dv: -0.6511033773422241, nwj: -0.7635180950164795\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.690261721611023, avg loss: 0.6930728636980057\n",
      "trial: 5, iter: 1000, curr loss: 0.6904635429382324, avg loss: 0.6926408169269562\n",
      "trial: 5, iter: 1500, curr loss: 0.7021732330322266, avg loss: 0.6918494738340378\n",
      "trial: 5, iter: 2000, curr loss: 0.6885952353477478, avg loss: 0.6914341462850571\n",
      "trial: 5, iter: 2500, curr loss: 0.6966838836669922, avg loss: 0.6908370679616929\n",
      "trial: 5, iter: 3000, curr loss: 0.6826525926589966, avg loss: 0.6875249131917953\n",
      "trial: 5, iter: 3500, curr loss: 0.6830744743347168, avg loss: 0.6849003401994705\n",
      "trial: 5, iter: 4000, curr loss: 0.6737480759620667, avg loss: 0.681748799443245\n",
      "trial: 5, iter: 4500, curr loss: 0.6703348755836487, avg loss: 0.679669405579567\n",
      "trial: 5, iter: 5000, curr loss: 0.6634769439697266, avg loss: 0.6748395028114319\n",
      "trial: 5, iter: 5500, curr loss: 0.6676639318466187, avg loss: 0.6724915922880172\n",
      "trial: 5, iter: 6000, curr loss: 0.6898183822631836, avg loss: 0.6681372318267822\n",
      "trial: 5, ldr: -0.25431904196739197, dv: -0.7437422275543213, nwj: -0.8856940269470215\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.22736516296863557\n",
      "\tdv: -0.658739173412323\n",
      "\tnwj: -0.7719340562820435\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6927026510238647, avg loss: 0.693092846274376\n",
      "trial: 1, iter: 1000, curr loss: 0.6830697059631348, avg loss: 0.6925737932920456\n",
      "trial: 1, iter: 1500, curr loss: 0.6922527551651001, avg loss: 0.6919358469247818\n",
      "trial: 1, iter: 2000, curr loss: 0.694995641708374, avg loss: 0.6917860614061355\n",
      "trial: 1, iter: 2500, curr loss: 0.7001810073852539, avg loss: 0.6907795069217681\n",
      "trial: 1, iter: 3000, curr loss: 0.6924725770950317, avg loss: 0.6891964020729064\n",
      "trial: 1, iter: 3500, curr loss: 0.7053909301757812, avg loss: 0.6873943538665771\n",
      "trial: 1, iter: 4000, curr loss: 0.6774202585220337, avg loss: 0.6840505131483078\n",
      "trial: 1, iter: 4500, curr loss: 0.6989795565605164, avg loss: 0.6798084045648575\n",
      "trial: 1, iter: 5000, curr loss: 0.6648532152175903, avg loss: 0.6765053133964538\n",
      "trial: 1, iter: 5500, curr loss: 0.6529527902603149, avg loss: 0.6722060387134552\n",
      "trial: 1, iter: 6000, curr loss: 0.662756085395813, avg loss: 0.6672160640954972\n",
      "trial: 1, ldr: -0.23592710494995117, dv: -0.5699706077575684, nwj: -0.6325310468673706\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6927289962768555, avg loss: 0.6931114180088043\n",
      "trial: 2, iter: 1000, curr loss: 0.6969922780990601, avg loss: 0.6927572802305222\n",
      "trial: 2, iter: 1500, curr loss: 0.6924803256988525, avg loss: 0.6924437382221222\n",
      "trial: 2, iter: 2000, curr loss: 0.6884778738021851, avg loss: 0.6920007325410843\n",
      "trial: 2, iter: 2500, curr loss: 0.6985602378845215, avg loss: 0.6909271130561828\n",
      "trial: 2, iter: 3000, curr loss: 0.6891761422157288, avg loss: 0.6903739050626755\n",
      "trial: 2, iter: 3500, curr loss: 0.6970483660697937, avg loss: 0.6891726399660111\n",
      "trial: 2, iter: 4000, curr loss: 0.6908634305000305, avg loss: 0.6864045699834823\n",
      "trial: 2, iter: 4500, curr loss: 0.667464017868042, avg loss: 0.6832956136465073\n",
      "trial: 2, iter: 5000, curr loss: 0.6687746047973633, avg loss: 0.680033051609993\n",
      "trial: 2, iter: 5500, curr loss: 0.6749024987220764, avg loss: 0.6774454503059387\n",
      "trial: 2, iter: 6000, curr loss: 0.6472007036209106, avg loss: 0.6734114516973495\n",
      "trial: 2, ldr: -0.19219717383384705, dv: -0.5394850969314575, nwj: -0.6074212789535522\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6893322467803955, avg loss: 0.6929417853355407\n",
      "trial: 3, iter: 1000, curr loss: 0.688032329082489, avg loss: 0.6921068259477615\n",
      "trial: 3, iter: 1500, curr loss: 0.696672797203064, avg loss: 0.6913011645078659\n",
      "trial: 3, iter: 2000, curr loss: 0.6915878057479858, avg loss: 0.6914379025697708\n",
      "trial: 3, iter: 2500, curr loss: 0.6906266808509827, avg loss: 0.6904775750637054\n",
      "trial: 3, iter: 3000, curr loss: 0.6832572221755981, avg loss: 0.6887737897634506\n",
      "trial: 3, iter: 3500, curr loss: 0.6900784969329834, avg loss: 0.6857987443208694\n",
      "trial: 3, iter: 4000, curr loss: 0.6753900051116943, avg loss: 0.6831845202445984\n",
      "trial: 3, iter: 4500, curr loss: 0.7152355313301086, avg loss: 0.6784325762987137\n",
      "trial: 3, iter: 5000, curr loss: 0.6629728078842163, avg loss: 0.6722539089918137\n",
      "trial: 3, iter: 5500, curr loss: 0.6732743978500366, avg loss: 0.6671003761291504\n",
      "trial: 3, iter: 6000, curr loss: 0.6691805124282837, avg loss: 0.6639548090696334\n",
      "trial: 3, ldr: -0.3154470920562744, dv: -0.7732446789741516, nwj: -0.8960361480712891\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.690913200378418, avg loss: 0.6930873394012451\n",
      "trial: 4, iter: 1000, curr loss: 0.6935787200927734, avg loss: 0.6926085857152939\n",
      "trial: 4, iter: 1500, curr loss: 0.6890186667442322, avg loss: 0.6919359699487686\n",
      "trial: 4, iter: 2000, curr loss: 0.6807610988616943, avg loss: 0.6913215653896332\n",
      "trial: 4, iter: 2500, curr loss: 0.6872805953025818, avg loss: 0.6908690639734268\n",
      "trial: 4, iter: 3000, curr loss: 0.6779448986053467, avg loss: 0.6890415743589401\n",
      "trial: 4, iter: 3500, curr loss: 0.7072672247886658, avg loss: 0.6865101782083511\n",
      "trial: 4, iter: 4000, curr loss: 0.6846235990524292, avg loss: 0.6833867136240005\n",
      "trial: 4, iter: 4500, curr loss: 0.6974830627441406, avg loss: 0.6805543262958527\n",
      "trial: 4, iter: 5000, curr loss: 0.6778055429458618, avg loss: 0.6745891325473785\n",
      "trial: 4, iter: 5500, curr loss: 0.669654369354248, avg loss: 0.6700878981351852\n",
      "trial: 4, iter: 6000, curr loss: 0.6902192831039429, avg loss: 0.6641397535800934\n",
      "trial: 4, ldr: -0.21099023520946503, dv: -0.6841744184494019, nwj: -0.8160871267318726\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6856462359428406, avg loss: 0.6929190340042114\n",
      "trial: 5, iter: 1000, curr loss: 0.6982480883598328, avg loss: 0.6924405030012131\n",
      "trial: 5, iter: 1500, curr loss: 0.7007140517234802, avg loss: 0.6921165877580643\n",
      "trial: 5, iter: 2000, curr loss: 0.693486750125885, avg loss: 0.6910956969261169\n",
      "trial: 5, iter: 2500, curr loss: 0.6851767301559448, avg loss: 0.6897068796157837\n",
      "trial: 5, iter: 3000, curr loss: 0.6927282810211182, avg loss: 0.6880588248968125\n",
      "trial: 5, iter: 3500, curr loss: 0.6834286451339722, avg loss: 0.6857764661312103\n",
      "trial: 5, iter: 4000, curr loss: 0.6791859269142151, avg loss: 0.6830632597208023\n",
      "trial: 5, iter: 4500, curr loss: 0.6865752935409546, avg loss: 0.6791139733791351\n",
      "trial: 5, iter: 5000, curr loss: 0.6490147113800049, avg loss: 0.674523128271103\n",
      "trial: 5, iter: 5500, curr loss: 0.6504822969436646, avg loss: 0.6708695293664932\n",
      "trial: 5, iter: 6000, curr loss: 0.6703246235847473, avg loss: 0.6656461253166198\n",
      "trial: 5, ldr: -0.294918030500412, dv: -0.6952029466629028, nwj: -0.7871679067611694\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.24989592730998994\n",
      "\tdv: -0.6524155497550964\n",
      "\tnwj: -0.7478487014770507\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6907122135162354, avg loss: 0.6930308015346527\n",
      "trial: 1, iter: 1000, curr loss: 0.6917779445648193, avg loss: 0.6927012581825256\n",
      "trial: 1, iter: 1500, curr loss: 0.7073410153388977, avg loss: 0.6922533663511277\n",
      "trial: 1, iter: 2000, curr loss: 0.6856595277786255, avg loss: 0.6917128388881684\n",
      "trial: 1, iter: 2500, curr loss: 0.6944018006324768, avg loss: 0.691396688580513\n",
      "trial: 1, iter: 3000, curr loss: 0.7028425931930542, avg loss: 0.6907067462205887\n",
      "trial: 1, iter: 3500, curr loss: 0.694756269454956, avg loss: 0.6890495499372482\n",
      "trial: 1, iter: 4000, curr loss: 0.6786217093467712, avg loss: 0.6880075670480729\n",
      "trial: 1, iter: 4500, curr loss: 0.6853111982345581, avg loss: 0.6850543936491013\n",
      "trial: 1, iter: 5000, curr loss: 0.6740069389343262, avg loss: 0.6828807176351547\n",
      "trial: 1, iter: 5500, curr loss: 0.720845639705658, avg loss: 0.6790349310636521\n",
      "trial: 1, iter: 6000, curr loss: 0.6855645775794983, avg loss: 0.6739425897598267\n",
      "trial: 1, ldr: -0.22112974524497986, dv: -0.5070505142211914, nwj: -0.5521167516708374\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6912212371826172, avg loss: 0.6931340008974075\n",
      "trial: 2, iter: 1000, curr loss: 0.6948255300521851, avg loss: 0.69258997797966\n",
      "trial: 2, iter: 1500, curr loss: 0.6978520750999451, avg loss: 0.6923066117763519\n",
      "trial: 2, iter: 2000, curr loss: 0.6965451240539551, avg loss: 0.6920604224205017\n",
      "trial: 2, iter: 2500, curr loss: 0.6823289394378662, avg loss: 0.6911927577257156\n",
      "trial: 2, iter: 3000, curr loss: 0.6922460794448853, avg loss: 0.6897481268644333\n",
      "trial: 2, iter: 3500, curr loss: 0.667665958404541, avg loss: 0.6886092567443848\n",
      "trial: 2, iter: 4000, curr loss: 0.6917399168014526, avg loss: 0.6866379678249359\n",
      "trial: 2, iter: 4500, curr loss: 0.6573114395141602, avg loss: 0.6829172525405883\n",
      "trial: 2, iter: 5000, curr loss: 0.6414456963539124, avg loss: 0.6790065602064133\n",
      "trial: 2, iter: 5500, curr loss: 0.6582555174827576, avg loss: 0.6744391689300537\n",
      "trial: 2, iter: 6000, curr loss: 0.670722484588623, avg loss: 0.6699984385967255\n",
      "trial: 2, ldr: -0.18116611242294312, dv: -0.6308225393295288, nwj: -0.7489395141601562\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.693158745765686, avg loss: 0.6932032572031022\n",
      "trial: 3, iter: 1000, curr loss: 0.6843770742416382, avg loss: 0.6926275535821914\n",
      "trial: 3, iter: 1500, curr loss: 0.6919493675231934, avg loss: 0.6923861877918244\n",
      "trial: 3, iter: 2000, curr loss: 0.6962528824806213, avg loss: 0.6918880800008774\n",
      "trial: 3, iter: 2500, curr loss: 0.692119836807251, avg loss: 0.6908863254785538\n",
      "trial: 3, iter: 3000, curr loss: 0.6943491697311401, avg loss: 0.6899143017530441\n",
      "trial: 3, iter: 3500, curr loss: 0.6998353004455566, avg loss: 0.687326294541359\n",
      "trial: 3, iter: 4000, curr loss: 0.6760171055793762, avg loss: 0.6843098896741867\n",
      "trial: 3, iter: 4500, curr loss: 0.6870585680007935, avg loss: 0.6806161184310913\n",
      "trial: 3, iter: 5000, curr loss: 0.7149863243103027, avg loss: 0.6773760329484939\n",
      "trial: 3, iter: 5500, curr loss: 0.7081447839736938, avg loss: 0.6732288122177124\n",
      "trial: 3, iter: 6000, curr loss: 0.7045952677726746, avg loss: 0.6671808706521988\n",
      "trial: 3, ldr: -0.1530601680278778, dv: -0.7775154113769531, nwj: -1.0202884674072266\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6917808055877686, avg loss: 0.6932648794651032\n",
      "trial: 4, iter: 1000, curr loss: 0.6918264627456665, avg loss: 0.69263589656353\n",
      "trial: 4, iter: 1500, curr loss: 0.690325140953064, avg loss: 0.6922283570766449\n",
      "trial: 4, iter: 2000, curr loss: 0.6828307509422302, avg loss: 0.691597048163414\n",
      "trial: 4, iter: 2500, curr loss: 0.6871364712715149, avg loss: 0.6907268513441086\n",
      "trial: 4, iter: 3000, curr loss: 0.6811345219612122, avg loss: 0.6886704454421997\n",
      "trial: 4, iter: 3500, curr loss: 0.6615111827850342, avg loss: 0.6864556174278259\n",
      "trial: 4, iter: 4000, curr loss: 0.681063175201416, avg loss: 0.6836040989160538\n",
      "trial: 4, iter: 4500, curr loss: 0.6632336974143982, avg loss: 0.6788698613643647\n",
      "trial: 4, iter: 5000, curr loss: 0.6560689210891724, avg loss: 0.6742644031047821\n",
      "trial: 4, iter: 5500, curr loss: 0.6571053266525269, avg loss: 0.6682688167095184\n",
      "trial: 4, iter: 6000, curr loss: 0.6919615864753723, avg loss: 0.6626421296596527\n",
      "trial: 4, ldr: -0.25984230637550354, dv: -0.7789671421051025, nwj: -0.9403985738754272\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.687992513179779, avg loss: 0.6931525026559829\n",
      "trial: 5, iter: 1000, curr loss: 0.7020742893218994, avg loss: 0.6927191494703293\n",
      "trial: 5, iter: 1500, curr loss: 0.6949468851089478, avg loss: 0.6924221477508545\n",
      "trial: 5, iter: 2000, curr loss: 0.6884514093399048, avg loss: 0.6919219282865524\n",
      "trial: 5, iter: 2500, curr loss: 0.692065417766571, avg loss: 0.6911122472286224\n",
      "trial: 5, iter: 3000, curr loss: 0.6905550360679626, avg loss: 0.6899819285869598\n",
      "trial: 5, iter: 3500, curr loss: 0.6794525384902954, avg loss: 0.6871416240930557\n",
      "trial: 5, iter: 4000, curr loss: 0.6933700442314148, avg loss: 0.6852560700178146\n",
      "trial: 5, iter: 4500, curr loss: 0.6885085105895996, avg loss: 0.6819735131263733\n",
      "trial: 5, iter: 5000, curr loss: 0.6357275247573853, avg loss: 0.6774474707841873\n",
      "trial: 5, iter: 5500, curr loss: 0.697186291217804, avg loss: 0.6730908983945847\n",
      "trial: 5, iter: 6000, curr loss: 0.6869487762451172, avg loss: 0.6681040375232696\n",
      "trial: 5, ldr: -0.24111641943454742, dv: -0.689150869846344, nwj: -0.8063490390777588\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.21126295030117034\n",
      "\tdv: -0.676701295375824\n",
      "\tnwj: -0.8136184692382813\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6974165439605713, avg loss: 0.6930908223390579\n",
      "trial: 1, iter: 1000, curr loss: 0.6881455183029175, avg loss: 0.6926400973796845\n",
      "trial: 1, iter: 1500, curr loss: 0.691875159740448, avg loss: 0.6922938181161881\n",
      "trial: 1, iter: 2000, curr loss: 0.6884046196937561, avg loss: 0.6919864888191223\n",
      "trial: 1, iter: 2500, curr loss: 0.6904336810112, avg loss: 0.6910395436286926\n",
      "trial: 1, iter: 3000, curr loss: 0.698632001876831, avg loss: 0.6895872790813447\n",
      "trial: 1, iter: 3500, curr loss: 0.6785292625427246, avg loss: 0.6883116612434387\n",
      "trial: 1, iter: 4000, curr loss: 0.6916484832763672, avg loss: 0.6853969525098801\n",
      "trial: 1, iter: 4500, curr loss: 0.6879022717475891, avg loss: 0.6827894275188446\n",
      "trial: 1, iter: 5000, curr loss: 0.6878869533538818, avg loss: 0.6808737066984176\n",
      "trial: 1, iter: 5500, curr loss: 0.6861255764961243, avg loss: 0.6779878866672516\n",
      "trial: 1, iter: 6000, curr loss: 0.6572192907333374, avg loss: 0.6738049763441086\n",
      "trial: 1, ldr: -0.1618749350309372, dv: -0.5263487100601196, nwj: -0.6016310453414917\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6958366632461548, avg loss: 0.6932373349666595\n",
      "trial: 2, iter: 1000, curr loss: 0.6879082322120667, avg loss: 0.6926549414396286\n",
      "trial: 2, iter: 1500, curr loss: 0.6991404294967651, avg loss: 0.69222385263443\n",
      "trial: 2, iter: 2000, curr loss: 0.681839108467102, avg loss: 0.6916321699619293\n",
      "trial: 2, iter: 2500, curr loss: 0.6858667731285095, avg loss: 0.6902538554668427\n",
      "trial: 2, iter: 3000, curr loss: 0.6851680874824524, avg loss: 0.6884048496484756\n",
      "trial: 2, iter: 3500, curr loss: 0.6832676529884338, avg loss: 0.6859190518856049\n",
      "trial: 2, iter: 4000, curr loss: 0.6664725542068481, avg loss: 0.6838698201179504\n",
      "trial: 2, iter: 4500, curr loss: 0.7042775750160217, avg loss: 0.6789761494398117\n",
      "trial: 2, iter: 5000, curr loss: 0.6800104379653931, avg loss: 0.6761253771781921\n",
      "trial: 2, iter: 5500, curr loss: 0.64577317237854, avg loss: 0.6711854236125946\n",
      "trial: 2, iter: 6000, curr loss: 0.6954787969589233, avg loss: 0.6664735578298568\n",
      "trial: 2, ldr: -0.21374846994876862, dv: -0.7118090391159058, nwj: -0.859275221824646\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6920134425163269, avg loss: 0.6932262322902679\n",
      "trial: 3, iter: 1000, curr loss: 0.6974257230758667, avg loss: 0.6925730528831482\n",
      "trial: 3, iter: 1500, curr loss: 0.6861202716827393, avg loss: 0.6923779363632202\n",
      "trial: 3, iter: 2000, curr loss: 0.6826546788215637, avg loss: 0.6918958530426026\n",
      "trial: 3, iter: 2500, curr loss: 0.6967567205429077, avg loss: 0.6902952716350556\n",
      "trial: 3, iter: 3000, curr loss: 0.6827560663223267, avg loss: 0.6885142351388931\n",
      "trial: 3, iter: 3500, curr loss: 0.6768355965614319, avg loss: 0.6859754855632783\n",
      "trial: 3, iter: 4000, curr loss: 0.6722960472106934, avg loss: 0.6841364616155624\n",
      "trial: 3, iter: 4500, curr loss: 0.6780863404273987, avg loss: 0.6801693456172944\n",
      "trial: 3, iter: 5000, curr loss: 0.6731937527656555, avg loss: 0.6746127135753631\n",
      "trial: 3, iter: 5500, curr loss: 0.6426870822906494, avg loss: 0.6724230983257293\n",
      "trial: 3, iter: 6000, curr loss: 0.6846239566802979, avg loss: 0.6673007007837296\n",
      "trial: 3, ldr: -0.1670594960451126, dv: -0.7400210499763489, nwj: -0.9405711889266968\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6925954222679138, avg loss: 0.6930923533439636\n",
      "trial: 4, iter: 1000, curr loss: 0.6937670111656189, avg loss: 0.6928095928430558\n",
      "trial: 4, iter: 1500, curr loss: 0.6896876692771912, avg loss: 0.6925186682939529\n",
      "trial: 4, iter: 2000, curr loss: 0.6813911199569702, avg loss: 0.6922306513786316\n",
      "trial: 4, iter: 2500, curr loss: 0.6999156475067139, avg loss: 0.6914780855178833\n",
      "trial: 4, iter: 3000, curr loss: 0.6819979548454285, avg loss: 0.6911530191898346\n",
      "trial: 4, iter: 3500, curr loss: 0.6941879987716675, avg loss: 0.6883911769390106\n",
      "trial: 4, iter: 4000, curr loss: 0.687343418598175, avg loss: 0.6867144178152085\n",
      "trial: 4, iter: 4500, curr loss: 0.6607644557952881, avg loss: 0.6826189439296723\n",
      "trial: 4, iter: 5000, curr loss: 0.6692246198654175, avg loss: 0.6806243005990982\n",
      "trial: 4, iter: 5500, curr loss: 0.7014157772064209, avg loss: 0.6757835427522659\n",
      "trial: 4, iter: 6000, curr loss: 0.6515862941741943, avg loss: 0.6724305241107941\n",
      "trial: 4, ldr: -0.2389916330575943, dv: -0.5022767186164856, nwj: -0.540189266204834\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6845470070838928, avg loss: 0.6928941127061844\n",
      "trial: 5, iter: 1000, curr loss: 0.7036328911781311, avg loss: 0.6926354576349258\n",
      "trial: 5, iter: 1500, curr loss: 0.6914311647415161, avg loss: 0.6925372766256332\n",
      "trial: 5, iter: 2000, curr loss: 0.6909089088439941, avg loss: 0.6920520713329316\n",
      "trial: 5, iter: 2500, curr loss: 0.6907150149345398, avg loss: 0.6919388099908829\n",
      "trial: 5, iter: 3000, curr loss: 0.6783522367477417, avg loss: 0.6908167471885681\n",
      "trial: 5, iter: 3500, curr loss: 0.6934785842895508, avg loss: 0.6893854233026504\n",
      "trial: 5, iter: 4000, curr loss: 0.6833713054656982, avg loss: 0.6873874599933625\n",
      "trial: 5, iter: 4500, curr loss: 0.6705865859985352, avg loss: 0.684049829363823\n",
      "trial: 5, iter: 5000, curr loss: 0.6567065715789795, avg loss: 0.6799343048334122\n",
      "trial: 5, iter: 5500, curr loss: 0.6553828716278076, avg loss: 0.6765392544269562\n",
      "trial: 5, iter: 6000, curr loss: 0.7016070485115051, avg loss: 0.6715091667175292\n",
      "trial: 5, ldr: -0.2609138488769531, dv: -0.6191108226776123, nwj: -0.6916612386703491\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.20851767659187317\n",
      "\tdv: -0.6199132680892945\n",
      "\tnwj: -0.7266655921936035\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6906627416610718, avg loss: 0.693012612938881\n",
      "trial: 1, iter: 1000, curr loss: 0.6913110017776489, avg loss: 0.6926880499124527\n",
      "trial: 1, iter: 1500, curr loss: 0.6982558965682983, avg loss: 0.6923365205526352\n",
      "trial: 1, iter: 2000, curr loss: 0.6921036243438721, avg loss: 0.6923731074333191\n",
      "trial: 1, iter: 2500, curr loss: 0.6892566084861755, avg loss: 0.6909979680776596\n",
      "trial: 1, iter: 3000, curr loss: 0.6981329917907715, avg loss: 0.6906010582447052\n",
      "trial: 1, iter: 3500, curr loss: 0.6818705797195435, avg loss: 0.6883373876810074\n",
      "trial: 1, iter: 4000, curr loss: 0.690369725227356, avg loss: 0.686069243311882\n",
      "trial: 1, iter: 4500, curr loss: 0.6823194622993469, avg loss: 0.6832960209846497\n",
      "trial: 1, iter: 5000, curr loss: 0.7010847330093384, avg loss: 0.6793556274175644\n",
      "trial: 1, iter: 5500, curr loss: 0.6689505577087402, avg loss: 0.6766395665407181\n",
      "trial: 1, iter: 6000, curr loss: 0.6709507703781128, avg loss: 0.6748279002904892\n",
      "trial: 1, ldr: -0.17189405858516693, dv: -0.5326394438743591, nwj: -0.6062922477722168\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6879061460494995, avg loss: 0.6930559221506118\n",
      "trial: 2, iter: 1000, curr loss: 0.6914901733398438, avg loss: 0.6927709884643555\n",
      "trial: 2, iter: 1500, curr loss: 0.6915080547332764, avg loss: 0.6918558108806611\n",
      "trial: 2, iter: 2000, curr loss: 0.6936522722244263, avg loss: 0.6917101204395294\n",
      "trial: 2, iter: 2500, curr loss: 0.6953107118606567, avg loss: 0.6899024118185043\n",
      "trial: 2, iter: 3000, curr loss: 0.6938852071762085, avg loss: 0.6878951054811477\n",
      "trial: 2, iter: 3500, curr loss: 0.6699343323707581, avg loss: 0.6852114301919937\n",
      "trial: 2, iter: 4000, curr loss: 0.6622602939605713, avg loss: 0.6813785902261734\n",
      "trial: 2, iter: 4500, curr loss: 0.6703702211380005, avg loss: 0.6772685694694519\n",
      "trial: 2, iter: 5000, curr loss: 0.6508609652519226, avg loss: 0.674232803940773\n",
      "trial: 2, iter: 5500, curr loss: 0.6974138021469116, avg loss: 0.6703044635057449\n",
      "trial: 2, iter: 6000, curr loss: 0.6571436524391174, avg loss: 0.6661527271270752\n",
      "trial: 2, ldr: -0.26688700914382935, dv: -0.7846886515617371, nwj: -0.945220947265625\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6940521001815796, avg loss: 0.693273266196251\n",
      "trial: 3, iter: 1000, curr loss: 0.6928534507751465, avg loss: 0.6922521171569824\n",
      "trial: 3, iter: 1500, curr loss: 0.6940030455589294, avg loss: 0.692304782152176\n",
      "trial: 3, iter: 2000, curr loss: 0.694873034954071, avg loss: 0.691945814371109\n",
      "trial: 3, iter: 2500, curr loss: 0.6967346668243408, avg loss: 0.6909927142858505\n",
      "trial: 3, iter: 3000, curr loss: 0.6875292062759399, avg loss: 0.6895326063632965\n",
      "trial: 3, iter: 3500, curr loss: 0.6942833662033081, avg loss: 0.6873456252813339\n",
      "trial: 3, iter: 4000, curr loss: 0.699289083480835, avg loss: 0.6847782318592072\n",
      "trial: 3, iter: 4500, curr loss: 0.6693132519721985, avg loss: 0.6823345503807068\n",
      "trial: 3, iter: 5000, curr loss: 0.6697593331336975, avg loss: 0.6789565796852112\n",
      "trial: 3, iter: 5500, curr loss: 0.6636111736297607, avg loss: 0.6751674176454544\n",
      "trial: 3, iter: 6000, curr loss: 0.6871483325958252, avg loss: 0.6711311509609222\n",
      "trial: 3, ldr: -0.25091439485549927, dv: -0.5480611324310303, nwj: -0.5969271659851074\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6971302032470703, avg loss: 0.6930319101810455\n",
      "trial: 4, iter: 1000, curr loss: 0.6953333020210266, avg loss: 0.6927764395475388\n",
      "trial: 4, iter: 1500, curr loss: 0.6908591985702515, avg loss: 0.6922258509397506\n",
      "trial: 4, iter: 2000, curr loss: 0.6879772543907166, avg loss: 0.6921649761199952\n",
      "trial: 4, iter: 2500, curr loss: 0.6987740993499756, avg loss: 0.6914962218999863\n",
      "trial: 4, iter: 3000, curr loss: 0.6985883712768555, avg loss: 0.6901268264055253\n",
      "trial: 4, iter: 3500, curr loss: 0.6911131143569946, avg loss: 0.6882023407220841\n",
      "trial: 4, iter: 4000, curr loss: 0.662499189376831, avg loss: 0.685846983551979\n",
      "trial: 4, iter: 4500, curr loss: 0.6994908452033997, avg loss: 0.6834903371334076\n",
      "trial: 4, iter: 5000, curr loss: 0.6910437941551208, avg loss: 0.6795753872394562\n",
      "trial: 4, iter: 5500, curr loss: 0.6756629943847656, avg loss: 0.6751266504526139\n",
      "trial: 4, iter: 6000, curr loss: 0.6734259128570557, avg loss: 0.6714350652694702\n",
      "trial: 4, ldr: -0.20229589939117432, dv: -0.6204432249069214, nwj: -0.721440315246582\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6916127800941467, avg loss: 0.6930818755626679\n",
      "trial: 5, iter: 1000, curr loss: 0.6946606636047363, avg loss: 0.6928624954223633\n",
      "trial: 5, iter: 1500, curr loss: 0.6922390460968018, avg loss: 0.6922971080541611\n",
      "trial: 5, iter: 2000, curr loss: 0.6836251020431519, avg loss: 0.6918686286211014\n",
      "trial: 5, iter: 2500, curr loss: 0.6879870891571045, avg loss: 0.6909749481678009\n",
      "trial: 5, iter: 3000, curr loss: 0.707240104675293, avg loss: 0.689513802409172\n",
      "trial: 5, iter: 3500, curr loss: 0.6843036413192749, avg loss: 0.686816484093666\n",
      "trial: 5, iter: 4000, curr loss: 0.6779031157493591, avg loss: 0.6836969199180603\n",
      "trial: 5, iter: 4500, curr loss: 0.688935399055481, avg loss: 0.6803874473571777\n",
      "trial: 5, iter: 5000, curr loss: 0.6956329345703125, avg loss: 0.6770417065620422\n",
      "trial: 5, iter: 5500, curr loss: 0.6604846715927124, avg loss: 0.672367991566658\n",
      "trial: 5, iter: 6000, curr loss: 0.6518754959106445, avg loss: 0.6687963047027587\n",
      "trial: 5, ldr: -0.29895785450935364, dv: -0.6622521281242371, nwj: -0.7370167970657349\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.2381898432970047\n",
      "\tdv: -0.629616916179657\n",
      "\tnwj: -0.7213794946670532\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6903551816940308, avg loss: 0.6930775907039642\n",
      "trial: 1, iter: 1000, curr loss: 0.6972570419311523, avg loss: 0.6925889129638672\n",
      "trial: 1, iter: 1500, curr loss: 0.6912564635276794, avg loss: 0.6925207738876342\n",
      "trial: 1, iter: 2000, curr loss: 0.6914644241333008, avg loss: 0.691946316242218\n",
      "trial: 1, iter: 2500, curr loss: 0.6915956139564514, avg loss: 0.6915275307893753\n",
      "trial: 1, iter: 3000, curr loss: 0.6898486614227295, avg loss: 0.6909076478481293\n",
      "trial: 1, iter: 3500, curr loss: 0.6820218563079834, avg loss: 0.6887555527687073\n",
      "trial: 1, iter: 4000, curr loss: 0.6695092916488647, avg loss: 0.6862999352216721\n",
      "trial: 1, iter: 4500, curr loss: 0.678924560546875, avg loss: 0.6829499398469925\n",
      "trial: 1, iter: 5000, curr loss: 0.6899065971374512, avg loss: 0.6793939325809478\n",
      "trial: 1, iter: 5500, curr loss: 0.63843834400177, avg loss: 0.6748605797290802\n",
      "trial: 1, iter: 6000, curr loss: 0.6846771836280823, avg loss: 0.6716435765027999\n",
      "trial: 1, ldr: -0.12431240826845169, dv: -0.5071879029273987, nwj: -0.5908077955245972\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6905884742736816, avg loss: 0.6932457280158997\n",
      "trial: 2, iter: 1000, curr loss: 0.6906180381774902, avg loss: 0.6928181430101394\n",
      "trial: 2, iter: 1500, curr loss: 0.6934853792190552, avg loss: 0.6923523839712143\n",
      "trial: 2, iter: 2000, curr loss: 0.6787393689155579, avg loss: 0.6922911002635955\n",
      "trial: 2, iter: 2500, curr loss: 0.6855688691139221, avg loss: 0.6914387546777725\n",
      "trial: 2, iter: 3000, curr loss: 0.6882445812225342, avg loss: 0.690338713645935\n",
      "trial: 2, iter: 3500, curr loss: 0.6784728169441223, avg loss: 0.6881982324123382\n",
      "trial: 2, iter: 4000, curr loss: 0.6927732229232788, avg loss: 0.6866175664663314\n",
      "trial: 2, iter: 4500, curr loss: 0.6834372282028198, avg loss: 0.682857773065567\n",
      "trial: 2, iter: 5000, curr loss: 0.6716693639755249, avg loss: 0.6797410858869553\n",
      "trial: 2, iter: 5500, curr loss: 0.6864321231842041, avg loss: 0.675825347661972\n",
      "trial: 2, iter: 6000, curr loss: 0.6709631085395813, avg loss: 0.6717310953140259\n",
      "trial: 2, ldr: -0.13834114372730255, dv: -0.6860201358795166, nwj: -0.867576003074646\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6898007392883301, avg loss: 0.6928686125278473\n",
      "trial: 3, iter: 1000, curr loss: 0.695826530456543, avg loss: 0.6926644603013993\n",
      "trial: 3, iter: 1500, curr loss: 0.6940611600875854, avg loss: 0.6920644444227219\n",
      "trial: 3, iter: 2000, curr loss: 0.6882873177528381, avg loss: 0.6918041951656342\n",
      "trial: 3, iter: 2500, curr loss: 0.6926093101501465, avg loss: 0.6909463549852372\n",
      "trial: 3, iter: 3000, curr loss: 0.7023607492446899, avg loss: 0.6895057171583175\n",
      "trial: 3, iter: 3500, curr loss: 0.6733289957046509, avg loss: 0.6873300271034241\n",
      "trial: 3, iter: 4000, curr loss: 0.6839369535446167, avg loss: 0.6845126755237579\n",
      "trial: 3, iter: 4500, curr loss: 0.6767401695251465, avg loss: 0.680366730093956\n",
      "trial: 3, iter: 5000, curr loss: 0.6723901033401489, avg loss: 0.6769045363664628\n",
      "trial: 3, iter: 5500, curr loss: 0.6426116228103638, avg loss: 0.6738813843727112\n",
      "trial: 3, iter: 6000, curr loss: 0.6569268703460693, avg loss: 0.6703783160448075\n",
      "trial: 3, ldr: -0.21797308325767517, dv: -0.6719282865524292, nwj: -0.7925006151199341\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6890549063682556, avg loss: 0.6931230027675629\n",
      "trial: 4, iter: 1000, curr loss: 0.6848823428153992, avg loss: 0.6926643301248551\n",
      "trial: 4, iter: 1500, curr loss: 0.6886011362075806, avg loss: 0.6923358489274979\n",
      "trial: 4, iter: 2000, curr loss: 0.6868957281112671, avg loss: 0.6922138750553131\n",
      "trial: 4, iter: 2500, curr loss: 0.6851072907447815, avg loss: 0.6916870840787888\n",
      "trial: 4, iter: 3000, curr loss: 0.6929229497909546, avg loss: 0.6910184090137481\n",
      "trial: 4, iter: 3500, curr loss: 0.695174515247345, avg loss: 0.6896931130886078\n",
      "trial: 4, iter: 4000, curr loss: 0.690766453742981, avg loss: 0.6884046788215638\n",
      "trial: 4, iter: 4500, curr loss: 0.7097676396369934, avg loss: 0.6870470207929611\n",
      "trial: 4, iter: 5000, curr loss: 0.6897364854812622, avg loss: 0.6837504817247391\n",
      "trial: 4, iter: 5500, curr loss: 0.7019139528274536, avg loss: 0.6794139231443406\n",
      "trial: 4, iter: 6000, curr loss: 0.7022703886032104, avg loss: 0.6760956352949142\n",
      "trial: 4, ldr: -0.08671017736196518, dv: -0.44250959157943726, nwj: -0.5140314102172852\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6884404420852661, avg loss: 0.6930472729206085\n",
      "trial: 5, iter: 1000, curr loss: 0.6971542835235596, avg loss: 0.6927770382165909\n",
      "trial: 5, iter: 1500, curr loss: 0.693152904510498, avg loss: 0.6926152894496918\n",
      "trial: 5, iter: 2000, curr loss: 0.6944137215614319, avg loss: 0.6920991580486298\n",
      "trial: 5, iter: 2500, curr loss: 0.7052597403526306, avg loss: 0.6916708792448044\n",
      "trial: 5, iter: 3000, curr loss: 0.6834715604782104, avg loss: 0.6905626192092895\n",
      "trial: 5, iter: 3500, curr loss: 0.6903314590454102, avg loss: 0.6889111770391464\n",
      "trial: 5, iter: 4000, curr loss: 0.6795651316642761, avg loss: 0.6855003768205643\n",
      "trial: 5, iter: 4500, curr loss: 0.6862810850143433, avg loss: 0.6822140499353408\n",
      "trial: 5, iter: 5000, curr loss: 0.6726172566413879, avg loss: 0.6790981912612915\n",
      "trial: 5, iter: 5500, curr loss: 0.6811472177505493, avg loss: 0.6741183700561524\n",
      "trial: 5, iter: 6000, curr loss: 0.6903680562973022, avg loss: 0.6701866420507431\n",
      "trial: 5, ldr: -0.2192116379737854, dv: -0.6469119191169739, nwj: -0.7529380321502686\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.157309690117836\n",
      "\tdv: -0.5909115672111511\n",
      "\tnwj: -0.7035707712173462\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6879165172576904, avg loss: 0.6931054250001908\n",
      "trial: 1, iter: 1000, curr loss: 0.6910536289215088, avg loss: 0.6926468133926391\n",
      "trial: 1, iter: 1500, curr loss: 0.6935944557189941, avg loss: 0.6923693054914475\n",
      "trial: 1, iter: 2000, curr loss: 0.69701087474823, avg loss: 0.6921081491708756\n",
      "trial: 1, iter: 2500, curr loss: 0.694850742816925, avg loss: 0.6918937375545502\n",
      "trial: 1, iter: 3000, curr loss: 0.6854232549667358, avg loss: 0.6904126908779145\n",
      "trial: 1, iter: 3500, curr loss: 0.6940958499908447, avg loss: 0.688438579916954\n",
      "trial: 1, iter: 4000, curr loss: 0.6675847172737122, avg loss: 0.6860474753379822\n",
      "trial: 1, iter: 4500, curr loss: 0.6761174201965332, avg loss: 0.6843390740156173\n",
      "trial: 1, iter: 5000, curr loss: 0.6753481030464172, avg loss: 0.6809900249242783\n",
      "trial: 1, iter: 5500, curr loss: 0.6941428780555725, avg loss: 0.6783392899036408\n",
      "trial: 1, iter: 6000, curr loss: 0.6832609176635742, avg loss: 0.6742526279687882\n",
      "trial: 1, ldr: -0.23084695637226105, dv: -0.4461037814617157, nwj: -0.4710273742675781\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6960668563842773, avg loss: 0.6930794751644135\n",
      "trial: 2, iter: 1000, curr loss: 0.6986572742462158, avg loss: 0.6922309325933457\n",
      "trial: 2, iter: 1500, curr loss: 0.6925585269927979, avg loss: 0.6923483608961105\n",
      "trial: 2, iter: 2000, curr loss: 0.6918759346008301, avg loss: 0.6921092491149903\n",
      "trial: 2, iter: 2500, curr loss: 0.690444827079773, avg loss: 0.6914324668645859\n",
      "trial: 2, iter: 3000, curr loss: 0.6894431114196777, avg loss: 0.6895515413284302\n",
      "trial: 2, iter: 3500, curr loss: 0.6828197240829468, avg loss: 0.6876502783298493\n",
      "trial: 2, iter: 4000, curr loss: 0.6905003786087036, avg loss: 0.6848939297199249\n",
      "trial: 2, iter: 4500, curr loss: 0.7049734592437744, avg loss: 0.6821909854412079\n",
      "trial: 2, iter: 5000, curr loss: 0.663789689540863, avg loss: 0.6773343402147293\n",
      "trial: 2, iter: 5500, curr loss: 0.6877390146255493, avg loss: 0.6736737595796585\n",
      "trial: 2, iter: 6000, curr loss: 0.6924726366996765, avg loss: 0.6691215687990189\n",
      "trial: 2, ldr: -0.21771487593650818, dv: -0.6287460923194885, nwj: -0.7260873317718506\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6925882697105408, avg loss: 0.6931940140724182\n",
      "trial: 3, iter: 1000, curr loss: 0.6882590055465698, avg loss: 0.6926611397266388\n",
      "trial: 3, iter: 1500, curr loss: 0.6909653544425964, avg loss: 0.6925059232711792\n",
      "trial: 3, iter: 2000, curr loss: 0.688819169998169, avg loss: 0.6914613962173461\n",
      "trial: 3, iter: 2500, curr loss: 0.6834235787391663, avg loss: 0.6905408573150634\n",
      "trial: 3, iter: 3000, curr loss: 0.6868283748626709, avg loss: 0.6887660356760025\n",
      "trial: 3, iter: 3500, curr loss: 0.6812448501586914, avg loss: 0.6854292203187943\n",
      "trial: 3, iter: 4000, curr loss: 0.6809967756271362, avg loss: 0.6829856375455856\n",
      "trial: 3, iter: 4500, curr loss: 0.6795147061347961, avg loss: 0.6785062466859817\n",
      "trial: 3, iter: 5000, curr loss: 0.6824487447738647, avg loss: 0.6756670725345612\n",
      "trial: 3, iter: 5500, curr loss: 0.6604580879211426, avg loss: 0.6693171364068985\n",
      "trial: 3, iter: 6000, curr loss: 0.6698004007339478, avg loss: 0.6672016187906266\n",
      "trial: 3, ldr: -0.30706626176834106, dv: -0.7951464653015137, nwj: -0.9362518787384033\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6946933269500732, avg loss: 0.6931385157108307\n",
      "trial: 4, iter: 1000, curr loss: 0.6871142387390137, avg loss: 0.6926103041172028\n",
      "trial: 4, iter: 1500, curr loss: 0.6892750263214111, avg loss: 0.6921255437135696\n",
      "trial: 4, iter: 2000, curr loss: 0.688390851020813, avg loss: 0.691881746172905\n",
      "trial: 4, iter: 2500, curr loss: 0.6980948448181152, avg loss: 0.6907615251541138\n",
      "trial: 4, iter: 3000, curr loss: 0.6853939294815063, avg loss: 0.6884400812387467\n",
      "trial: 4, iter: 3500, curr loss: 0.681240439414978, avg loss: 0.6861762778759003\n",
      "trial: 4, iter: 4000, curr loss: 0.6753268241882324, avg loss: 0.6823112545013428\n",
      "trial: 4, iter: 4500, curr loss: 0.6831225156784058, avg loss: 0.6796563739776611\n",
      "trial: 4, iter: 5000, curr loss: 0.6831457614898682, avg loss: 0.6744543347358704\n",
      "trial: 4, iter: 5500, curr loss: 0.6867852210998535, avg loss: 0.6698899142742157\n",
      "trial: 4, iter: 6000, curr loss: 0.6872762441635132, avg loss: 0.6648189121484757\n",
      "trial: 4, ldr: -0.2680416405200958, dv: -0.6717805862426758, nwj: -0.7654546499252319\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6917282342910767, avg loss: 0.6928566333055496\n",
      "trial: 5, iter: 1000, curr loss: 0.6858980059623718, avg loss: 0.6923561633825303\n",
      "trial: 5, iter: 1500, curr loss: 0.690605878829956, avg loss: 0.69216748046875\n",
      "trial: 5, iter: 2000, curr loss: 0.691910982131958, avg loss: 0.6909317002296448\n",
      "trial: 5, iter: 2500, curr loss: 0.6883199214935303, avg loss: 0.6897166858911514\n",
      "trial: 5, iter: 3000, curr loss: 0.6968116164207458, avg loss: 0.6878194506168366\n",
      "trial: 5, iter: 3500, curr loss: 0.6771295070648193, avg loss: 0.6848390666246414\n",
      "trial: 5, iter: 4000, curr loss: 0.6824301481246948, avg loss: 0.6823240932226181\n",
      "trial: 5, iter: 4500, curr loss: 0.6332539319992065, avg loss: 0.6767160339355469\n",
      "trial: 5, iter: 5000, curr loss: 0.6815335750579834, avg loss: 0.672251989364624\n",
      "trial: 5, iter: 5500, curr loss: 0.6430975198745728, avg loss: 0.669429215669632\n",
      "trial: 5, iter: 6000, curr loss: 0.6557453870773315, avg loss: 0.6659307212829589\n",
      "trial: 5, ldr: -0.30227214097976685, dv: -0.9872295260429382, nwj: -1.2859594821929932\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.2651883751153946\n",
      "\tdv: -0.7058012902736663\n",
      "\tnwj: -0.8369561433792114\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6882456541061401, avg loss: 0.6931399737596512\n",
      "trial: 1, iter: 1000, curr loss: 0.695909321308136, avg loss: 0.6925311849117279\n",
      "trial: 1, iter: 1500, curr loss: 0.6987366676330566, avg loss: 0.6919244443178176\n",
      "trial: 1, iter: 2000, curr loss: 0.6896414160728455, avg loss: 0.6907895032167435\n",
      "trial: 1, iter: 2500, curr loss: 0.6935441493988037, avg loss: 0.6897879121303558\n",
      "trial: 1, iter: 3000, curr loss: 0.7024593353271484, avg loss: 0.6883409658670425\n",
      "trial: 1, iter: 3500, curr loss: 0.6904265284538269, avg loss: 0.6857238328456878\n",
      "trial: 1, iter: 4000, curr loss: 0.6953625679016113, avg loss: 0.6833137899637223\n",
      "trial: 1, iter: 4500, curr loss: 0.669941246509552, avg loss: 0.6799159997701645\n",
      "trial: 1, iter: 5000, curr loss: 0.6821906566619873, avg loss: 0.675106999874115\n",
      "trial: 1, iter: 5500, curr loss: 0.6674818992614746, avg loss: 0.672221742272377\n",
      "trial: 1, iter: 6000, curr loss: 0.669133722782135, avg loss: 0.6661330279111862\n",
      "trial: 1, ldr: -0.17793536186218262, dv: -0.626448392868042, nwj: -0.7439172267913818\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.692275881767273, avg loss: 0.6930436751842499\n",
      "trial: 2, iter: 1000, curr loss: 0.684404194355011, avg loss: 0.6926255047321319\n",
      "trial: 2, iter: 1500, curr loss: 0.6893249154090881, avg loss: 0.6922441188097\n",
      "trial: 2, iter: 2000, curr loss: 0.6978727579116821, avg loss: 0.6916863958835602\n",
      "trial: 2, iter: 2500, curr loss: 0.6893569231033325, avg loss: 0.6912206927537918\n",
      "trial: 2, iter: 3000, curr loss: 0.6775088906288147, avg loss: 0.6899255234003067\n",
      "trial: 2, iter: 3500, curr loss: 0.6945736408233643, avg loss: 0.6887712028026581\n",
      "trial: 2, iter: 4000, curr loss: 0.6730416417121887, avg loss: 0.6855091251134873\n",
      "trial: 2, iter: 4500, curr loss: 0.7037614583969116, avg loss: 0.6838868844509125\n",
      "trial: 2, iter: 5000, curr loss: 0.6882311105728149, avg loss: 0.6789497337341308\n",
      "trial: 2, iter: 5500, curr loss: 0.665552020072937, avg loss: 0.6751889986991882\n",
      "trial: 2, iter: 6000, curr loss: 0.6860889792442322, avg loss: 0.6703412094116211\n",
      "trial: 2, ldr: -0.2339698076248169, dv: -0.5013880133628845, nwj: -0.5405565500259399\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6924718618392944, avg loss: 0.6932288199663162\n",
      "trial: 3, iter: 1000, curr loss: 0.700536847114563, avg loss: 0.6926133850812912\n",
      "trial: 3, iter: 1500, curr loss: 0.6888389587402344, avg loss: 0.691795964717865\n",
      "trial: 3, iter: 2000, curr loss: 0.6890984773635864, avg loss: 0.6914778958559036\n",
      "trial: 3, iter: 2500, curr loss: 0.6767615675926208, avg loss: 0.6901008886098862\n",
      "trial: 3, iter: 3000, curr loss: 0.7048512101173401, avg loss: 0.6894386398792267\n",
      "trial: 3, iter: 3500, curr loss: 0.7063747644424438, avg loss: 0.687052034854889\n",
      "trial: 3, iter: 4000, curr loss: 0.6812224388122559, avg loss: 0.6842825878858566\n",
      "trial: 3, iter: 4500, curr loss: 0.6678121089935303, avg loss: 0.6816822023391723\n",
      "trial: 3, iter: 5000, curr loss: 0.6796367764472961, avg loss: 0.6791000081300735\n",
      "trial: 3, iter: 5500, curr loss: 0.6519982814788818, avg loss: 0.6735556468963623\n",
      "trial: 3, iter: 6000, curr loss: 0.6514894962310791, avg loss: 0.6689154670238495\n",
      "trial: 3, ldr: -0.21734702587127686, dv: -0.5971977710723877, nwj: -0.6794133186340332\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6944456100463867, avg loss: 0.6931962631940841\n",
      "trial: 4, iter: 1000, curr loss: 0.6957105398178101, avg loss: 0.6925383491516113\n",
      "trial: 4, iter: 1500, curr loss: 0.6940242052078247, avg loss: 0.6921649301052093\n",
      "trial: 4, iter: 2000, curr loss: 0.6940515041351318, avg loss: 0.6914893267154694\n",
      "trial: 4, iter: 2500, curr loss: 0.69720458984375, avg loss: 0.6908581368923187\n",
      "trial: 4, iter: 3000, curr loss: 0.6861995458602905, avg loss: 0.689024965763092\n",
      "trial: 4, iter: 3500, curr loss: 0.6819918751716614, avg loss: 0.6875720969438552\n",
      "trial: 4, iter: 4000, curr loss: 0.6786752343177795, avg loss: 0.6832024259567261\n",
      "trial: 4, iter: 4500, curr loss: 0.6853167414665222, avg loss: 0.6809274635314941\n",
      "trial: 4, iter: 5000, curr loss: 0.6512478590011597, avg loss: 0.6769036107063293\n",
      "trial: 4, iter: 5500, curr loss: 0.6776005029678345, avg loss: 0.6751834424734116\n",
      "trial: 4, iter: 6000, curr loss: 0.6905116438865662, avg loss: 0.6700709233283997\n",
      "trial: 4, ldr: -0.262596994638443, dv: -0.6271480321884155, nwj: -0.7024643421173096\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6908756494522095, avg loss: 0.6929992101192475\n",
      "trial: 5, iter: 1000, curr loss: 0.6921688318252563, avg loss: 0.6927385840415955\n",
      "trial: 5, iter: 1500, curr loss: 0.6931960582733154, avg loss: 0.6923596850633621\n",
      "trial: 5, iter: 2000, curr loss: 0.6921038627624512, avg loss: 0.6916720134019851\n",
      "trial: 5, iter: 2500, curr loss: 0.6911264657974243, avg loss: 0.6904854992628098\n",
      "trial: 5, iter: 3000, curr loss: 0.6883785724639893, avg loss: 0.6893002413511277\n",
      "trial: 5, iter: 3500, curr loss: 0.6829443573951721, avg loss: 0.6869906272888183\n",
      "trial: 5, iter: 4000, curr loss: 0.6792799234390259, avg loss: 0.6837625405788421\n",
      "trial: 5, iter: 4500, curr loss: 0.6954056024551392, avg loss: 0.6799074351787567\n",
      "trial: 5, iter: 5000, curr loss: 0.6799267530441284, avg loss: 0.6764369746446609\n",
      "trial: 5, iter: 5500, curr loss: 0.6595728993415833, avg loss: 0.672458468079567\n",
      "trial: 5, iter: 6000, curr loss: 0.6789271235466003, avg loss: 0.6664796441793441\n",
      "trial: 5, ldr: -0.1799275428056717, dv: -0.626895010471344, nwj: -0.7434910535812378\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.21435534656047822\n",
      "\tdv: -0.5958154439926148\n",
      "\tnwj: -0.6819684982299805\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6883827447891235, avg loss: 0.6931833534240722\n",
      "trial: 1, iter: 1000, curr loss: 0.6949923634529114, avg loss: 0.6926580482721328\n",
      "trial: 1, iter: 1500, curr loss: 0.68808913230896, avg loss: 0.6920114251375198\n",
      "trial: 1, iter: 2000, curr loss: 0.694028377532959, avg loss: 0.692075278878212\n",
      "trial: 1, iter: 2500, curr loss: 0.6955616474151611, avg loss: 0.6911426686048507\n",
      "trial: 1, iter: 3000, curr loss: 0.7001432180404663, avg loss: 0.6899807654619217\n",
      "trial: 1, iter: 3500, curr loss: 0.6925393342971802, avg loss: 0.6877684768438339\n",
      "trial: 1, iter: 4000, curr loss: 0.6730763912200928, avg loss: 0.684884052157402\n",
      "trial: 1, iter: 4500, curr loss: 0.6515759229660034, avg loss: 0.6817846395969391\n",
      "trial: 1, iter: 5000, curr loss: 0.6491057276725769, avg loss: 0.6762120906114578\n",
      "trial: 1, iter: 5500, curr loss: 0.6607648134231567, avg loss: 0.6698705259561538\n",
      "trial: 1, iter: 6000, curr loss: 0.6636497974395752, avg loss: 0.6663444472551345\n",
      "trial: 1, ldr: -0.2925467789173126, dv: -0.6785025596618652, nwj: -0.7635663747787476\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6890369653701782, avg loss: 0.693251008272171\n",
      "trial: 2, iter: 1000, curr loss: 0.6918590068817139, avg loss: 0.6927715507745743\n",
      "trial: 2, iter: 1500, curr loss: 0.6890214085578918, avg loss: 0.6926212102174759\n",
      "trial: 2, iter: 2000, curr loss: 0.6957358121871948, avg loss: 0.6919870113134384\n",
      "trial: 2, iter: 2500, curr loss: 0.6884759664535522, avg loss: 0.6918044279813766\n",
      "trial: 2, iter: 3000, curr loss: 0.6807466745376587, avg loss: 0.6900428949594498\n",
      "trial: 2, iter: 3500, curr loss: 0.6962404847145081, avg loss: 0.6889119639396667\n",
      "trial: 2, iter: 4000, curr loss: 0.6701210737228394, avg loss: 0.686246309876442\n",
      "trial: 2, iter: 4500, curr loss: 0.705825924873352, avg loss: 0.6827868925333023\n",
      "trial: 2, iter: 5000, curr loss: 0.6700533628463745, avg loss: 0.6788142442703247\n",
      "trial: 2, iter: 5500, curr loss: 0.6661518216133118, avg loss: 0.6750373200178147\n",
      "trial: 2, iter: 6000, curr loss: 0.6726776361465454, avg loss: 0.6709727424383164\n",
      "trial: 2, ldr: -0.21776898205280304, dv: -0.5495616793632507, nwj: -0.6112329959869385\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6937106251716614, avg loss: 0.6933036340475083\n",
      "trial: 3, iter: 1000, curr loss: 0.7009751796722412, avg loss: 0.6927062696218491\n",
      "trial: 3, iter: 1500, curr loss: 0.6839642524719238, avg loss: 0.6925711070299149\n",
      "trial: 3, iter: 2000, curr loss: 0.6924203038215637, avg loss: 0.6922091273069382\n",
      "trial: 3, iter: 2500, curr loss: 0.6862049102783203, avg loss: 0.6911621807813645\n",
      "trial: 3, iter: 3000, curr loss: 0.6955015659332275, avg loss: 0.6899898586273193\n",
      "trial: 3, iter: 3500, curr loss: 0.6975561380386353, avg loss: 0.6877135130167007\n",
      "trial: 3, iter: 4000, curr loss: 0.706833004951477, avg loss: 0.6849039818048477\n",
      "trial: 3, iter: 4500, curr loss: 0.6658559441566467, avg loss: 0.6802342909574509\n",
      "trial: 3, iter: 5000, curr loss: 0.7000376582145691, avg loss: 0.6759439455270767\n",
      "trial: 3, iter: 5500, curr loss: 0.6635756492614746, avg loss: 0.6738313083648682\n",
      "trial: 3, iter: 6000, curr loss: 0.6422230005264282, avg loss: 0.670622831583023\n",
      "trial: 3, ldr: -0.1947442591190338, dv: -0.7238941192626953, nwj: -0.8922328948974609\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6952331066131592, avg loss: 0.6930617696046829\n",
      "trial: 4, iter: 1000, curr loss: 0.689723789691925, avg loss: 0.6927364641427993\n",
      "trial: 4, iter: 1500, curr loss: 0.679721474647522, avg loss: 0.692393042564392\n",
      "trial: 4, iter: 2000, curr loss: 0.695082426071167, avg loss: 0.6919392838478088\n",
      "trial: 4, iter: 2500, curr loss: 0.6879023313522339, avg loss: 0.6908520696163177\n",
      "trial: 4, iter: 3000, curr loss: 0.6898758411407471, avg loss: 0.6900551253557206\n",
      "trial: 4, iter: 3500, curr loss: 0.6838960647583008, avg loss: 0.6874728876352311\n",
      "trial: 4, iter: 4000, curr loss: 0.7125211954116821, avg loss: 0.6852871142625808\n",
      "trial: 4, iter: 4500, curr loss: 0.665402889251709, avg loss: 0.6814183560609818\n",
      "trial: 4, iter: 5000, curr loss: 0.6765865087509155, avg loss: 0.6773765598535537\n",
      "trial: 4, iter: 5500, curr loss: 0.6547471284866333, avg loss: 0.6739945265054703\n",
      "trial: 4, iter: 6000, curr loss: 0.6712496280670166, avg loss: 0.6709279150962829\n",
      "trial: 4, ldr: -0.19618964195251465, dv: -0.6168834567070007, nwj: -0.7192075252532959\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6936683654785156, avg loss: 0.6931662844419479\n",
      "trial: 5, iter: 1000, curr loss: 0.6895886659622192, avg loss: 0.692735174536705\n",
      "trial: 5, iter: 1500, curr loss: 0.6918615102767944, avg loss: 0.6922300066947937\n",
      "trial: 5, iter: 2000, curr loss: 0.6928872466087341, avg loss: 0.6915006271600723\n",
      "trial: 5, iter: 2500, curr loss: 0.6847532391548157, avg loss: 0.6906529188156127\n",
      "trial: 5, iter: 3000, curr loss: 0.703687310218811, avg loss: 0.6881218111515045\n",
      "trial: 5, iter: 3500, curr loss: 0.6772794723510742, avg loss: 0.686132489323616\n",
      "trial: 5, iter: 4000, curr loss: 0.6910192370414734, avg loss: 0.6823898893594742\n",
      "trial: 5, iter: 4500, curr loss: 0.6719896793365479, avg loss: 0.6796362913846969\n",
      "trial: 5, iter: 5000, curr loss: 0.6735442876815796, avg loss: 0.6744780782461166\n",
      "trial: 5, iter: 5500, curr loss: 0.6335840225219727, avg loss: 0.6682006078958511\n",
      "trial: 5, iter: 6000, curr loss: 0.6499044299125671, avg loss: 0.6650772159099578\n",
      "trial: 5, ldr: -0.2910628914833069, dv: -0.7277010083198547, nwj: -0.8385589122772217\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.2384625107049942\n",
      "\tdv: -0.6593085646629333\n",
      "\tnwj: -0.764959740638733\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6877986192703247, avg loss: 0.6929825609922409\n",
      "trial: 1, iter: 1000, curr loss: 0.6985418796539307, avg loss: 0.6924232414960861\n",
      "trial: 1, iter: 1500, curr loss: 0.6906698942184448, avg loss: 0.6921197221279144\n",
      "trial: 1, iter: 2000, curr loss: 0.6906207799911499, avg loss: 0.6921586960554122\n",
      "trial: 1, iter: 2500, curr loss: 0.6849281787872314, avg loss: 0.6910978368520737\n",
      "trial: 1, iter: 3000, curr loss: 0.677826464176178, avg loss: 0.6894663578271866\n",
      "trial: 1, iter: 3500, curr loss: 0.6902154088020325, avg loss: 0.6876300243139267\n",
      "trial: 1, iter: 4000, curr loss: 0.6739399433135986, avg loss: 0.6849714345932006\n",
      "trial: 1, iter: 4500, curr loss: 0.6807317733764648, avg loss: 0.6816430208683014\n",
      "trial: 1, iter: 5000, curr loss: 0.6780836582183838, avg loss: 0.6780198731422424\n",
      "trial: 1, iter: 5500, curr loss: 0.6998758316040039, avg loss: 0.675005959868431\n",
      "trial: 1, iter: 6000, curr loss: 0.6783562302589417, avg loss: 0.6709298788309097\n",
      "trial: 1, ldr: -0.21700817346572876, dv: -0.5214624404907227, nwj: -0.5728931427001953\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6894826889038086, avg loss: 0.6926662795543671\n",
      "trial: 2, iter: 1000, curr loss: 0.6963324546813965, avg loss: 0.6925792793035507\n",
      "trial: 2, iter: 1500, curr loss: 0.6938910484313965, avg loss: 0.6921942808628082\n",
      "trial: 2, iter: 2000, curr loss: 0.6890581846237183, avg loss: 0.69129829454422\n",
      "trial: 2, iter: 2500, curr loss: 0.7082393169403076, avg loss: 0.6907020672559738\n",
      "trial: 2, iter: 3000, curr loss: 0.6890162229537964, avg loss: 0.6904952634572983\n",
      "trial: 2, iter: 3500, curr loss: 0.6969120502471924, avg loss: 0.6876951676607131\n",
      "trial: 2, iter: 4000, curr loss: 0.7086029052734375, avg loss: 0.6838056471347809\n",
      "trial: 2, iter: 4500, curr loss: 0.6771026849746704, avg loss: 0.6803599475622177\n",
      "trial: 2, iter: 5000, curr loss: 0.6630789041519165, avg loss: 0.6766748661994935\n",
      "trial: 2, iter: 5500, curr loss: 0.6754902601242065, avg loss: 0.6728041554689408\n",
      "trial: 2, iter: 6000, curr loss: 0.6626577377319336, avg loss: 0.6684323147535324\n",
      "trial: 2, ldr: -0.2715388834476471, dv: -0.8023443222045898, nwj: -0.9718400239944458\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6944082379341125, avg loss: 0.6931581298112869\n",
      "trial: 3, iter: 1000, curr loss: 0.69190514087677, avg loss: 0.6924841188192368\n",
      "trial: 3, iter: 1500, curr loss: 0.6957956552505493, avg loss: 0.6918319255113602\n",
      "trial: 3, iter: 2000, curr loss: 0.6911153793334961, avg loss: 0.6913559089899063\n",
      "trial: 3, iter: 2500, curr loss: 0.6958757638931274, avg loss: 0.6900248031616211\n",
      "trial: 3, iter: 3000, curr loss: 0.6842069625854492, avg loss: 0.6876099462509155\n",
      "trial: 3, iter: 3500, curr loss: 0.6563930511474609, avg loss: 0.6835156248807908\n",
      "trial: 3, iter: 4000, curr loss: 0.6666359305381775, avg loss: 0.6802420582771301\n",
      "trial: 3, iter: 4500, curr loss: 0.6871095895767212, avg loss: 0.6773482033014298\n",
      "trial: 3, iter: 5000, curr loss: 0.6488688588142395, avg loss: 0.6716039222478867\n",
      "trial: 3, iter: 5500, curr loss: 0.6446686387062073, avg loss: 0.6669242007732391\n",
      "trial: 3, iter: 6000, curr loss: 0.6180911064147949, avg loss: 0.6606667287349701\n",
      "trial: 3, ldr: -0.28449490666389465, dv: -0.849560022354126, nwj: -1.0440573692321777\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6863994598388672, avg loss: 0.693089792251587\n",
      "trial: 4, iter: 1000, curr loss: 0.6968744993209839, avg loss: 0.6925732365846634\n",
      "trial: 4, iter: 1500, curr loss: 0.6929185390472412, avg loss: 0.6923867045640946\n",
      "trial: 4, iter: 2000, curr loss: 0.6932029724121094, avg loss: 0.691302088022232\n",
      "trial: 4, iter: 2500, curr loss: 0.7004128694534302, avg loss: 0.6908550529479981\n",
      "trial: 4, iter: 3000, curr loss: 0.6946091055870056, avg loss: 0.6895053681135178\n",
      "trial: 4, iter: 3500, curr loss: 0.6695175170898438, avg loss: 0.687891252040863\n",
      "trial: 4, iter: 4000, curr loss: 0.7072941064834595, avg loss: 0.6842433533668518\n",
      "trial: 4, iter: 4500, curr loss: 0.652228832244873, avg loss: 0.6825571663379669\n",
      "trial: 4, iter: 5000, curr loss: 0.6635797023773193, avg loss: 0.680007615685463\n",
      "trial: 4, iter: 5500, curr loss: 0.6728304028511047, avg loss: 0.6767157877683639\n",
      "trial: 4, iter: 6000, curr loss: 0.6714247465133667, avg loss: 0.671896467089653\n",
      "trial: 4, ldr: -0.2966676950454712, dv: -0.5447147488594055, nwj: -0.5781879425048828\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6890737414360046, avg loss: 0.6928860913515091\n",
      "trial: 5, iter: 1000, curr loss: 0.6893277764320374, avg loss: 0.6926614711284638\n",
      "trial: 5, iter: 1500, curr loss: 0.6928009986877441, avg loss: 0.6924940575361251\n",
      "trial: 5, iter: 2000, curr loss: 0.6950613856315613, avg loss: 0.6919129865169525\n",
      "trial: 5, iter: 2500, curr loss: 0.6953728795051575, avg loss: 0.6919238457679748\n",
      "trial: 5, iter: 3000, curr loss: 0.6817499399185181, avg loss: 0.6907095968723297\n",
      "trial: 5, iter: 3500, curr loss: 0.6983696222305298, avg loss: 0.6897873332500458\n",
      "trial: 5, iter: 4000, curr loss: 0.7132158279418945, avg loss: 0.6864368281364441\n",
      "trial: 5, iter: 4500, curr loss: 0.6887001991271973, avg loss: 0.684581348657608\n",
      "trial: 5, iter: 5000, curr loss: 0.6863482594490051, avg loss: 0.6811032117605209\n",
      "trial: 5, iter: 5500, curr loss: 0.6621642112731934, avg loss: 0.677145269393921\n",
      "trial: 5, iter: 6000, curr loss: 0.6874462366104126, avg loss: 0.6710097869634628\n",
      "trial: 5, ldr: -0.2502477765083313, dv: -0.5714255571365356, nwj: -0.6289985179901123\n",
      "################################################################\n",
      "\n",
      "final estimations first:\n",
      "\tldr: -0.2639914870262146\n",
      "\tdv: -0.6579014182090759\n",
      "\tnwj: -0.7591953992843627\n",
      "\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 0.6961565017700195, avg loss: 0.69305810546875\n",
      "trial: 1, iter: 1000, curr loss: 0.6896845102310181, avg loss: 0.6925140396356583\n",
      "trial: 1, iter: 1500, curr loss: 0.6927124857902527, avg loss: 0.6922598552703857\n",
      "trial: 1, iter: 2000, curr loss: 0.6892950534820557, avg loss: 0.6921160129308701\n",
      "trial: 1, iter: 2500, curr loss: 0.6993762254714966, avg loss: 0.6913012255430222\n",
      "trial: 1, iter: 3000, curr loss: 0.6897760629653931, avg loss: 0.6899715222120285\n",
      "trial: 1, iter: 3500, curr loss: 0.6813950538635254, avg loss: 0.6871330251693726\n",
      "trial: 1, iter: 4000, curr loss: 0.6604241132736206, avg loss: 0.6843287324905396\n",
      "trial: 1, iter: 4500, curr loss: 0.6768932342529297, avg loss: 0.6815535987615585\n",
      "trial: 1, iter: 5000, curr loss: 0.7222124338150024, avg loss: 0.6776650882959366\n",
      "trial: 1, iter: 5500, curr loss: 0.6719520092010498, avg loss: 0.6738236032724381\n",
      "trial: 1, iter: 6000, curr loss: 0.6547171473503113, avg loss: 0.6697894051074982\n",
      "trial: 1, ldr: -0.1605159193277359, dv: -0.6158426403999329, nwj: -0.7372043132781982\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 0.6876809597015381, avg loss: 0.6930819065570831\n",
      "trial: 2, iter: 1000, curr loss: 0.6879155039787292, avg loss: 0.6925361187458038\n",
      "trial: 2, iter: 1500, curr loss: 0.6893748044967651, avg loss: 0.6922631199359894\n",
      "trial: 2, iter: 2000, curr loss: 0.693115234375, avg loss: 0.6918609931468963\n",
      "trial: 2, iter: 2500, curr loss: 0.6966593861579895, avg loss: 0.6914704513549804\n",
      "trial: 2, iter: 3000, curr loss: 0.6965155601501465, avg loss: 0.6903897193670273\n",
      "trial: 2, iter: 3500, curr loss: 0.6967528462409973, avg loss: 0.6891746211051941\n",
      "trial: 2, iter: 4000, curr loss: 0.6697710752487183, avg loss: 0.6878125537633896\n",
      "trial: 2, iter: 4500, curr loss: 0.6797503232955933, avg loss: 0.684738889336586\n",
      "trial: 2, iter: 5000, curr loss: 0.6750285029411316, avg loss: 0.6825099041461945\n",
      "trial: 2, iter: 5500, curr loss: 0.6768732070922852, avg loss: 0.6798784208297729\n",
      "trial: 2, iter: 6000, curr loss: 0.7022577524185181, avg loss: 0.6739160177707673\n",
      "trial: 2, ldr: -0.1792324334383011, dv: -0.5009177327156067, nwj: -0.5586830377578735\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 0.6976447105407715, avg loss: 0.6929160083532333\n",
      "trial: 3, iter: 1000, curr loss: 0.6981152296066284, avg loss: 0.6923714898824692\n",
      "trial: 3, iter: 1500, curr loss: 0.6845079064369202, avg loss: 0.6918535476922989\n",
      "trial: 3, iter: 2000, curr loss: 0.6894924640655518, avg loss: 0.6913082342147827\n",
      "trial: 3, iter: 2500, curr loss: 0.6889614462852478, avg loss: 0.6902692650556564\n",
      "trial: 3, iter: 3000, curr loss: 0.6914750337600708, avg loss: 0.6892148170471192\n",
      "trial: 3, iter: 3500, curr loss: 0.6842539310455322, avg loss: 0.6872085692882538\n",
      "trial: 3, iter: 4000, curr loss: 0.6684859991073608, avg loss: 0.6844503732919693\n",
      "trial: 3, iter: 4500, curr loss: 0.6527470350265503, avg loss: 0.6812042282819748\n",
      "trial: 3, iter: 5000, curr loss: 0.6534363031387329, avg loss: 0.6791087518930435\n",
      "trial: 3, iter: 5500, curr loss: 0.7159824371337891, avg loss: 0.6750644702911377\n",
      "trial: 3, iter: 6000, curr loss: 0.660545289516449, avg loss: 0.6706402790546417\n",
      "trial: 3, ldr: -0.11846678704023361, dv: -0.5853261947631836, nwj: -0.7134438753128052\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 0.6983230113983154, avg loss: 0.6932067639827728\n",
      "trial: 4, iter: 1000, curr loss: 0.6989344358444214, avg loss: 0.6925930242538452\n",
      "trial: 4, iter: 1500, curr loss: 0.6889718174934387, avg loss: 0.6920701880455017\n",
      "trial: 4, iter: 2000, curr loss: 0.6970669031143188, avg loss: 0.6919610224962235\n",
      "trial: 4, iter: 2500, curr loss: 0.6854252815246582, avg loss: 0.6914174379110336\n",
      "trial: 4, iter: 3000, curr loss: 0.6919552087783813, avg loss: 0.6895824028253555\n",
      "trial: 4, iter: 3500, curr loss: 0.6871120929718018, avg loss: 0.6881192520856857\n",
      "trial: 4, iter: 4000, curr loss: 0.69459468126297, avg loss: 0.6848508007526398\n",
      "trial: 4, iter: 4500, curr loss: 0.6939656138420105, avg loss: 0.681801211476326\n",
      "trial: 4, iter: 5000, curr loss: 0.6663285493850708, avg loss: 0.6775044922828675\n",
      "trial: 4, iter: 5500, curr loss: 0.6647202372550964, avg loss: 0.6734270049333573\n",
      "trial: 4, iter: 6000, curr loss: 0.656612753868103, avg loss: 0.6677579008340836\n",
      "trial: 4, ldr: -0.24441058933734894, dv: -0.7244696617126465, nwj: -0.860580563545227\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 0.6939563751220703, avg loss: 0.6929639893770217\n",
      "trial: 5, iter: 1000, curr loss: 0.6860490441322327, avg loss: 0.6925023663043975\n",
      "trial: 5, iter: 1500, curr loss: 0.6920496821403503, avg loss: 0.691820235133171\n",
      "trial: 5, iter: 2000, curr loss: 0.6925712823867798, avg loss: 0.6913569592237473\n",
      "trial: 5, iter: 2500, curr loss: 0.6905686855316162, avg loss: 0.6900596935749054\n",
      "trial: 5, iter: 3000, curr loss: 0.6943752765655518, avg loss: 0.6891684336662293\n",
      "trial: 5, iter: 3500, curr loss: 0.6852124929428101, avg loss: 0.686915800690651\n",
      "trial: 5, iter: 4000, curr loss: 0.693096399307251, avg loss: 0.6840558114051819\n",
      "trial: 5, iter: 4500, curr loss: 0.6849216222763062, avg loss: 0.6803870078325271\n",
      "trial: 5, iter: 5000, curr loss: 0.6839795112609863, avg loss: 0.6762587895393372\n",
      "trial: 5, iter: 5500, curr loss: 0.6804282069206238, avg loss: 0.6737015731334687\n",
      "trial: 5, iter: 6000, curr loss: 0.6722193956375122, avg loss: 0.6713265873193741\n",
      "trial: 5, ldr: -0.1618279069662094, dv: -0.5228735208511353, nwj: -0.5966567993164062\n",
      "################################################################\n",
      "\n",
      "final estimations second:\n",
      "\tldr: -0.1728907272219658\n",
      "\tdv: -0.589885950088501\n",
      "\tnwj: -0.693313717842102\n"
     ]
    }
   ],
   "source": [
    "ldr_arr = []\n",
    "dv_arr = []\n",
    "nwj_arr = []\n",
    "for i in range(num_of_outer_iteration):\n",
    "    ldr, dv, nwj = ccmi_experiment(prime, data_range, num_of_samples, weight, feature_size, beta_arr, alpha_arr, para_param, priv_param, x_idx, y_idx, z_idx, hidden_size_arr, lr, num_of_mid_iteration, num_of_inner_iteration, batch_size, load_data='./data/catNon-lin-NI_19/data.20k.dz200.seed0.npy', save_avg=500)\n",
    "    ldr_arr.append(ldr)\n",
    "    dv_arr.append(dv)\n",
    "    nwj_arr.append(nwj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:35:53.669783785Z",
     "start_time": "2023-12-13T00:26:31.299001290Z"
    }
   },
   "id": "a207951bd578c28d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "ldr_mean = np.mean(np.asarray(ldr_arr))\n",
    "dv_mean = np.mean(np.asarray(dv_arr))\n",
    "nwj_mean = np.mean(np.asarray(nwj_arr))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:36:18.021668687Z",
     "start_time": "2023-12-13T00:36:18.016618463Z"
    }
   },
   "id": "2370e3d7f41b8a24"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.008342275470495219"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldr_mean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:36:18.787557088Z",
     "start_time": "2023-12-13T00:36:18.783266636Z"
    }
   },
   "id": "9af8b3b4388e92ac"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0.07763880968093873"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv_mean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:36:19.669196009Z",
     "start_time": "2023-12-13T00:36:19.665332822Z"
    }
   },
   "id": "9270cf10bb9cc011"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5836285758018493"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwj_mean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:36:20.523356993Z",
     "start_time": "2023-12-13T00:36:20.518580493Z"
    }
   },
   "id": "9484a60318daa232"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 1.3871957063674927, avg loss: 1.3864824514389038\n",
      "trial: 1, iter: 1000, curr loss: 1.3866695165634155, avg loss: 1.386362098455429\n",
      "trial: 1, iter: 1500, curr loss: 1.386459469795227, avg loss: 1.3863300409317016\n",
      "trial: 1, ldr: -0.003490493865683675\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 1.3867965936660767, avg loss: 1.386491316318512\n",
      "trial: 2, iter: 1000, curr loss: 1.3859913349151611, avg loss: 1.3863506689071656\n",
      "trial: 2, iter: 1500, curr loss: 1.3862322568893433, avg loss: 1.3863347554206848\n",
      "trial: 2, ldr: -0.009700164198875427\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 1.3852827548980713, avg loss: 1.3864503908157348\n",
      "trial: 3, iter: 1000, curr loss: 1.3861894607543945, avg loss: 1.3863485808372498\n",
      "trial: 3, iter: 1500, curr loss: 1.3867136240005493, avg loss: 1.3863305130004884\n",
      "trial: 3, ldr: -0.015043365769088268\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 1.386287808418274, avg loss: 1.3864330203533173\n",
      "trial: 4, iter: 1000, curr loss: 1.3866560459136963, avg loss: 1.3863330404758454\n",
      "trial: 4, iter: 1500, curr loss: 1.386169672012329, avg loss: 1.3863337962627411\n",
      "trial: 4, ldr: 0.0124486293643713\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 5, iter: 500, curr loss: 1.3863447904586792, avg loss: 1.3864354596138\n",
      "trial: 5, iter: 1000, curr loss: 1.3862882852554321, avg loss: 1.386378574848175\n",
      "trial: 5, iter: 1500, curr loss: 1.3863840103149414, avg loss: 1.3863343966007233\n",
      "trial: 5, ldr: 0.005840935744345188\n",
      "################################################################\n",
      "\n",
      "final estimations:\n",
      "\tldr: -0.0019888917449861765\n",
      "################################################################\n",
      "trial: 1, iter: 500, curr loss: 1.3862801790237427, avg loss: 1.3864824962615967\n",
      "trial: 1, iter: 1000, curr loss: 1.3858404159545898, avg loss: 1.3863415834903716\n",
      "trial: 1, iter: 1500, curr loss: 1.386281132698059, avg loss: 1.386339492559433\n",
      "trial: 1, ldr: -9.391062485519797e-05\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 2, iter: 500, curr loss: 1.3863929510116577, avg loss: 1.386472307920456\n",
      "trial: 2, iter: 1000, curr loss: 1.3864917755126953, avg loss: 1.386338040113449\n",
      "trial: 2, iter: 1500, curr loss: 1.3862007856369019, avg loss: 1.386328498363495\n",
      "trial: 2, ldr: 0.00970506016165018\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 3, iter: 500, curr loss: 1.3864372968673706, avg loss: 1.3864626812934875\n",
      "trial: 3, iter: 1000, curr loss: 1.386319637298584, avg loss: 1.3863454248905183\n",
      "trial: 3, iter: 1500, curr loss: 1.386443018913269, avg loss: 1.3863346633911133\n",
      "trial: 3, ldr: 0.0010865695076063275\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "trial: 4, iter: 500, curr loss: 1.3864110708236694, avg loss: 1.386502204656601\n",
      "trial: 4, iter: 1000, curr loss: 1.3863601684570312, avg loss: 1.3863783140182495\n",
      "trial: 4, iter: 1500, curr loss: 1.3863950967788696, avg loss: 1.386343472480774\n",
      "trial: 4, ldr: -0.003800058038905263\n",
      "################################################################\n",
      "\n",
      "################################################################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m ldr_arr_multi \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_of_outer_iteration):\n\u001B[0;32m----> 3\u001B[0m     ldr \u001B[38;5;241m=\u001B[39m \u001B[43mmulticlass_probabilistic_classifier_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_range\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_of_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpara_param\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpriv_param\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_size_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_of_mid_iteration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_of_inner_iteration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mload_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data/catNon-lin-NI_19/data.20k.dz200.seed0.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_avg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     ldr_arr_multi\u001B[38;5;241m.\u001B[39mappend(ldr)\n",
      "File \u001B[0;32m~/Desktop/umityigitbsrn/research/repos/mutual-information-between-secret-coeff-and-inversion-attack/probabilistic_classifier/experiment.py:144\u001B[0m, in \u001B[0;36mmulticlass_probabilistic_classifier_experiment\u001B[0;34m(prime, data_range, num_of_samples, weight, feature_size, beta_arr, alpha_arr, para_param, priv_param, x_idx, y_idx, z_idx, hidden_size_arr, lr, num_of_outer_iteration, num_of_inner_iteration, batch_size, save_avg, print_progress, return_loss, device, load_data)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m outer_iter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_of_outer_iteration):\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m################################################################\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    143\u001B[0m     (model, inner_running_loss, inner_running_loss_avg, num_of_joint, num_of_all_marginal,\n\u001B[0;32m--> 144\u001B[0m      num_of_marginal_y_joint_xz, num_of_marginal_x_joint_yz) \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_multiclass_classifier_v2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjoint_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mjoint_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mall_marginal_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mall_marginal_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mmarginal_x_joint_yz_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mmarginal_x_joint_yz_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mmarginal_y_joint_xz_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mmarginal_y_joint_xz_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mnum_input_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mhidden_size_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mnum_of_inner_iteration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mouter_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mprint_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprint_progress\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43msave_avg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_avg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m                                                                                              \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m     outer_running_loss\u001B[38;5;241m.\u001B[39mappend(inner_running_loss)\n\u001B[1;32m    161\u001B[0m     outer_running_loss_avg\u001B[38;5;241m.\u001B[39mappend(inner_running_loss_avg)\n",
      "File \u001B[0;32m~/Desktop/umityigitbsrn/research/repos/mutual-information-between-secret-coeff-and-inversion-attack/probabilistic_classifier/train.py:267\u001B[0m, in \u001B[0;36mtrain_multiclass_classifier_v2\u001B[0;34m(joint_data, joint_label, marginal_data, marginal_label, x_par_mar_data, x_par_mar_label, y_par_mar_data, y_par_mar_label, num_input_features, hidden_size_arr, lr, number_of_iterations, batch_size, outer_iter, save_avg, print_progress, device)\u001B[0m\n\u001B[1;32m    265\u001B[0m selected_joint \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(joint_data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], batch_size, replace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    266\u001B[0m selected_marginal \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(joint_data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], batch_size, replace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 267\u001B[0m selected_x_par_mar \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjoint_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    268\u001B[0m selected_y_par_mar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(joint_data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], batch_size, replace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    269\u001B[0m batch_data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([joint_data[selected_joint], marginal_data[selected_marginal],\n\u001B[1;32m    270\u001B[0m                             x_par_mar_data[selected_x_par_mar], y_par_mar_data[selected_y_par_mar]])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ldr_arr_multi = []\n",
    "for i in range(num_of_outer_iteration):\n",
    "    ldr = multiclass_probabilistic_classifier_experiment(prime, data_range, num_of_samples, weight, feature_size, beta_arr, alpha_arr, para_param, priv_param, x_idx, y_idx, z_idx, hidden_size_arr, lr, num_of_mid_iteration, num_of_inner_iteration, batch_size, load_data='./data/catNon-lin-NI_19/data.20k.dz200.seed0.npy', save_avg=500)\n",
    "    ldr_arr_multi.append(ldr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:40:02.421277139Z",
     "start_time": "2023-12-13T00:39:15.211337245Z"
    }
   },
   "id": "96ca868b1ad72df3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40128523051738735\n"
     ]
    }
   ],
   "source": [
    "ldr_multi_mean = np.mean(np.asarray(ldr_arr_multi))\n",
    "print(ldr_multi_mean)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T00:22:11.056588186Z",
     "start_time": "2023-12-13T00:22:11.050549235Z"
    }
   },
   "id": "284db425800f4c7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
